{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "    \n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "mnist_data = digits.images.reshape((n_samples, -1)).astype(float)\n",
    "labels = digits.target.astype(int)\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "onehot_labels = to_categorical(labels)\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, onehot_labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "n_features = X_train.shape[1]\n",
    "n_neurons_layer1 = 100\n",
    "n_neurons_layer2 = 50\n",
    "n_categories = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 52us/step\n",
      "Accuracy: 0.892\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_DNN(n_features, n_neurons_layer1, n_neurons_layer2, n_categories):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons_layer1, input_dim=n_features, activation='sigmoid'))\n",
    "    model.add(Dense(n_neurons_layer1, activation='sigmoid'))\n",
    "    model.add(Dense(n_categories, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_DNN(n_features, n_neurons_layer1, n_neurons_layer2, n_categories)\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f\" % scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "mnist_trainset = datasets.MNIST('../data/torch/mnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "mnist_testset = datasets.MNIST('../data/torch/mnist', train=False, download=True, transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DNNTorch(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "    ):\n",
    "        super(DNNTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_neurons_layer1)\n",
    "        self.fc2 = nn.Linear(n_neurons_layer1, n_neurons_layer2)\n",
    "        self.out = nn.Linear(n_neurons_layer2, n_categories)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.302\n",
      "[1,   200] loss: 2.295\n",
      "[1,   300] loss: 2.284\n",
      "[1,   400] loss: 2.267\n",
      "[1,   500] loss: 2.231\n",
      "[1,   600] loss: 2.135\n",
      "[2,   100] loss: 1.924\n",
      "[2,   200] loss: 1.652\n",
      "[2,   300] loss: 1.419\n",
      "[2,   400] loss: 1.251\n",
      "[2,   500] loss: 1.087\n",
      "[2,   600] loss: 0.951\n",
      "Finished Training\n",
      "Accuracy of the network on the test images: 75 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "n_features = 28*28\n",
    "epochs = 2\n",
    "\n",
    "DNN = DNNTorch(n_features, n_neurons_layer1, n_neurons_layer2, n_categories)\n",
    "DNN.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(DNN.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data, in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, n_features)\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = DNN(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % batch_size == batch_size - 1:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / batch_size))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, n_features)\n",
    "        outputs = DNN(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
