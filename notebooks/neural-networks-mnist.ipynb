{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "    \n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "mnist_data = digits.images.reshape((n_samples, -1))\n",
    "labels = digits.target\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_data,\n",
    "        Y_data,\n",
    "        epochs=1,\n",
    "        batch_size=100,\n",
    "        eta=1e-2,\n",
    "        lmbd=0.0,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "    ):\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.weights_layer1 = np.random.randn(self.n_features, self.n_neurons_layer1)\n",
    "        self.bias_layer1 = np.zeros(self.n_neurons_layer1)\n",
    "\n",
    "        self.weights_layer2 = np.random.randn(self.n_neurons_layer1, self.n_neurons_layer2)\n",
    "        self.bias_layer2 = np.zeros(self.n_neurons_layer2)\n",
    "\n",
    "        self.weights_output = np.random.randn(self.n_neurons_layer2, self.n_categories)\n",
    "        self.bias_output = np.zeros(self.n_categories)\n",
    "\n",
    "    def feed_forward(self):\n",
    "        self.z1 = np.dot(self.X_data, self.weights_layer1) + self.bias_layer1\n",
    "        self.a1 = expit(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights_layer2) + self.bias_layer2\n",
    "        self.a2 = expit(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.weights_output) + self.bias_output\n",
    "\n",
    "        exp_term = np.exp(self.z3)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "        z1 = np.dot(X, self.weights_layer1) + self.bias_layer1\n",
    "        a1 = expit(z1)\n",
    "\n",
    "        z2 = np.dot(a1, self.weights_layer2) + self.bias_layer2\n",
    "        a2 = expit(z2)\n",
    "\n",
    "        z3 = np.dot(a2, self.weights_output) + self.bias_output\n",
    "\n",
    "        exp_term = np.exp(z3)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backpropagation(self):\n",
    "        error_output = self.probabilities\n",
    "        error_output[range(self.n_inputs), self.Y_data] -= 1\n",
    "        error_layer2 = (np.dot(error_output, self.weights_output.T) * self.a2 * (1 - self.a2))\n",
    "        error_layer1 = (np.dot(error_layer2, self.weights_layer2.T) * self.a1 * (1 - self.a1))\n",
    "\n",
    "        self.weights_output_gradient = np.dot(self.a2.T, error_output)\n",
    "        self.bias_output_gradient = np.sum(error_output)\n",
    "\n",
    "        self.weights_layer2_gradient = np.dot(self.a1.T, error_layer2)\n",
    "        self.bias_layer2_gradient = np.sum(error_layer2)\n",
    "\n",
    "        self.weights_layer1_gradient = np.dot(self.X_data.T, error_layer1)\n",
    "        self.bias_layer1_gradient = np.sum(error_layer1)\n",
    "\n",
    "        if self.lmbd > 0.0:\n",
    "            self.weights_output_gradient += self.lmbd * self.weights_output\n",
    "            self.weights_layer2_gradient += self.lmbd * self.weights_layer2\n",
    "            self.weights_layer1_gradient += self.lmbd * self.weights_layer1\n",
    "\n",
    "        self.weights_output -= self.eta * self.weights_output_gradient\n",
    "        self.bias_output -= self.eta * self.bias_output_gradient\n",
    "        self.weights_layer2 -= self.eta * self.weights_layer2_gradient\n",
    "        self.bias_layer2 -= self.eta * self.bias_layer2_gradient\n",
    "        self.weights_layer1 -= self.eta * self.weights_layer1_gradient\n",
    "        self.bias_layer1 -= self.eta * self.bias_layer1_gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.n_inputs = self.X_data.shape[0]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "n_neurons_layer1 = 100\n",
    "n_neurons_layer2 = 50\n",
    "n_categories = 10\n",
    "\n",
    "eta_vals = np.logspace(-5, 0, 6)\n",
    "lmbd_vals = np.logspace(-5, 0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1e-05\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.09722222222222222\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.11388888888888889\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.1\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.15555555555555556\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.10833333333333334\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.16111111111111112\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.20555555555555555\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.19444444444444445\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.18888888888888888\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.18333333333333332\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.21666666666666667\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.6916666666666667\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.5833333333333334\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.5861111111111111\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.5361111111111111\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.6388888888888888\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.6305555555555555\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9111111111111111\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9166666666666666\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9111111111111111\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9166666666666666\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9222222222222223\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9333333333333333\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.10555555555555556\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.08611111111111111\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.09166666666666666\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.08611111111111111\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.1\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.10833333333333334\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = NeuralNetwork(X_train, Y_train, eta=eta, lmbd=lmbd, epochs=10, batch_size=batch_size,\n",
    "                                  n_neurons_layer1=n_neurons_layer1, n_neurons_layer2=n_neurons_layer2,\n",
    "                                  n_categories=n_categories)\n",
    "        dnn.train()\n",
    "        \n",
    "        DNN_numpy[i][j] = dnn\n",
    "        \n",
    "        test_predict = dnn.predict(X_test)\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1e-05\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.28055555555555556\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.09166666666666666\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.09722222222222222\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.1111111111111111\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.09166666666666666\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.19166666666666668\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.875\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.8777777777777778\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9166666666666666\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.875\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.8361111111111111\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9833333333333333\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9888888888888889\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9888888888888889\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9944444444444445\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9722222222222222\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9833333333333333\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9888888888888889\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9944444444444445\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9833333333333333\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9583333333333334\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.46111111111111114\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.08611111111111111\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.4666666666666667\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.5555555555555556\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.8777777777777778\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.85\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.1111111111111111\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.08611111111111111\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_neurons_layer1, n_neurons_layer2), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=100)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        \n",
    "        DNN_scikit[i][j] = dnn\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", dnn.score(X_test, Y_test))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_data,\n",
    "        Y_data,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_DNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.n_features), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_DNN(self):\n",
    "        with tf.name_scope('DNN'):\n",
    "            \n",
    "            # Fully connected layer 1\n",
    "            W_fc1 = self.weight_variable([self.n_features, self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            b_fc1 = self.bias_variable([self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            a_fc1 = tf.nn.sigmoid(tf.matmul(self.X, W_fc1) + b_fc1)\n",
    "            \n",
    "            # Fully connected layer 2\n",
    "            W_fc2 = self.weight_variable([self.n_neurons_layer1, self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            b_fc2 = self.bias_variable([self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            a_fc2 = tf.nn.sigmoid(tf.matmul(a_fc1, W_fc2) + b_fc2)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_layer2, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc2, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (100,) for Tensor 'data_15/Y_data:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f62aa50698df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             sess.run([DNN.loss, DNN.optimizer],\n\u001b[1;32m     13\u001b[0m                     feed_dict={DNN.X: batch_X,\n\u001b[0;32m---> 14\u001b[0;31m                                DNN.Y: batch_Y})\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1111\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1112\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (100,) for Tensor 'data_15/Y_data:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "DNN = DNNModel(X_train, Y_train, n_neurons_layer1, n_neurons_layer2, n_categories, eta=0.1)\n",
    "\n",
    "iterations = X_train.shape[0] // batch_size\n",
    "data_indices = np.arange(X_train.shape[0])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([DNN.loss, DNN.optimizer],\n",
    "                    feed_dict={DNN.X: batch_X,\n",
    "                               DNN.Y: batch_Y})\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
