{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks in numpy and scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Artificial neural networks are computational systems that can learn to perform tasks by considering examples,\n",
    "generally without being programmed with any task-specific rules. It is supposed to mimic a biological system, wherein neurons interact by sending signals in the form of mathematical functions between layers. All layers can contain an arbitrary number of neurons, and each connection is represented by a weight variable.  \n",
    "  \n",
    "In this tutorial we will build a feed-forward neural network, where information moves in only in direction:\n",
    "forward through the layers. Each neuron or *node* is represented by a circle, while arrows display the connections\n",
    "between the nodes and indicate the direction of information flow. Each node in a layer is connected to all nodes in the subsequent layer, which makes this a so-called *fully-connected* feed-forward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FFNN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)  \n",
    "Via [Wikipedia](https://en.wikipedia.org/wiki/File:Colored_neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "To follow this tutorial we require an installation of Python with the numerical package **numpy**, either:  \n",
    "1) Python 2.7.x  \n",
    "2) Python 3.5.x or greater  \n",
    "  \n",
    "With a version of Numpy 1.0.x or greater.  \n",
    "We will also use the packages **matplotlib**, **scikit-learn**, **Tensorflow** and **Keras**, though these are not strictly necessary.  \n",
    "To open and run this notebook you also need an installation of **IPython** and **Jupyter Notebook**.  \n",
    "  \n",
    "# Anaconda\n",
    "Anaconda is a free and open source Python and R distribution, that aims to simplify package management and deployment. Anaconda comes with more than 1000 data packages, as well as the Conda package and package and virtual environment manager. Anaconda is available on Linux, OS X and Windows systems, and contains nearly all prerequisite software, and comes highly recommended.  \n",
    "If Anaconda is installed you can install Tensorflow and Keras using:  \n",
    "  \n",
    "```conda install tensorflow```  \n",
    "```conda install keras```  \n",
    "  \n",
    "(You may run into minor problems with conflicting package versions)  \n",
    "  \n",
    "# Pip package manager\n",
    "If you do not wish to install Anaconda you may download Python from [here](https://www.python.org/downloads/),\n",
    "or you can use package managers like **brew**, **apt**, **pacman**,...  \n",
    "Python distributions come with their own package manager, **pip**, and once you have Python installed\n",
    "you can run the following command:  \n",
    "  \n",
    "```pip install numpy matplotlib scikit-learn ipython jupyter```  \n",
    "  \n",
    "To install Tensorflow follow the instructions [here](https://www.tensorflow.org/install/).  \n",
    "After you have installed tensorflow you can install keras:  \n",
    "  \n",
    "```pip install keras```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python](https://imgs.xkcd.com/comics/python_environment.png)  \n",
    "Via [xkcd](https://xkcd.com/1987/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "  \n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "  \n",
    "**\n",
    "1) Collect and pre-process data  \n",
    "2) Define model and architecture  \n",
    "3) Choose cost function and optimizer  \n",
    "4) Train the model  \n",
    "5) Evaluate model performance on test data  \n",
    "6) Adjust hyperparameters (if necessary, network architecture)\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Collect and pre-process data\n",
    "  \n",
    "In this tutorial we will be using the MNIST dataset, which is readily available through the **scikit-learn**\n",
    "package. You may also find it for example [here](http://yann.lecun.com/exdb/mnist/).  \n",
    "The **MNIST** (Modified National Institute of Standards and Technology) database is a large database\n",
    "of handwritten digits that is commonly used for training various image processing systems.  \n",
    "The MNIST dataset consists of 70 000 images of size 28x28 pixels, each labeled from 0 to 9.  \n",
    "  \n",
    "To feed data into a feed-forward neural network we need to represent the inputs as a feature matrix $X = [n_{inputs}, \n",
    "n_{features}]$.  \n",
    "Each row represents an **input**, in this case a handwritten digit, and each column represents a **feature**, in this case a pixel.  \n",
    "The correct answers, also known as **labels** or **targets** are represented as a 1D array of integers $Y = [n_{inputs}] = [5, 3, 1, 8,...]$.  \n",
    "  \n",
    "Say I wanted to build a neural network using supervised learning to predict Body-Mass Index (BMI) from\n",
    "measurements of height (in m)  \n",
    "and weight (in kg). If I had measurements of 5 people the feature matrix could be for example:  \n",
    "  \n",
    "$$ X = \\begin{bmatrix}\n",
    "1.85 & 81\\\\\n",
    "1.71 & 65\\\\\n",
    "1.95 & 103\\\\\n",
    "1.55 & 42\\\\\n",
    "1.63 & 56\n",
    "\\end{bmatrix} ,$$  \n",
    "  \n",
    "and the targets would be:  \n",
    "  \n",
    "$$ Y = (23.7, 22.2, 27.1, 17.5, 21.1) $$  \n",
    "  \n",
    "Since each input image is a 2D matrix, we need to flatten the image (i.e. \"unravel\" the 2D matrix into a 1D array)  \n",
    "to turn the data into a feature matrix. This means we lose all spatial information in the image, such as locality and translational invariance ([explanation](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo))  \n",
    "More complicated architectures such as Convolutional Neural Networks can take advantage\n",
    "of such information, and is most commonly applied when analyzing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = (n_inputs, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "labels = (n_inputs) = (1797,)\n",
      "X = (n_inputs, n_features) = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "#ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# display images in notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(\"inputs = (n_inputs, pixel_width, pixel_height) = \" + str(inputs.shape))\n",
    "print(\"labels = (n_inputs) = \" + str(labels.shape))\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(\"X = (n_inputs, n_features) = \" + str(inputs.shape))\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[random_indices[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test datasets\n",
    "\n",
    "Performing analysis before partitioning the dataset is a major error, that can lead to incorrect conclusions  \n",
    "(see \"Bias-Variance Tradeoff\", for example [here](https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/)).  \n",
    "  \n",
    "We will reserve $80 \\%$ of our dataset for training and $20 \\%$ for testing.  \n",
    "  \n",
    "It is important that the train and test datasets are drawn randomly from our dataset, to ensure\n",
    "no bias in the sampling.  \n",
    "Say you are taking measurements of weather data to predict the weather in the coming 5 days.\n",
    "You don't want to train your model on measurements taken from the hours 00.00 to 12.00, and then test it on data\n",
    "collected from 12.00 to 24.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# one-liner from scikit-learn library\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "# equivalently in numpy\n",
    "def train_test_split_numpy(inputs, labels, train_size, test_size):\n",
    "    n_inputs = len(inputs)\n",
    "    inputs_shuffled = inputs.copy()\n",
    "    labels_shuffled = labels.copy()\n",
    "    \n",
    "    np.random.shuffle(inputs_shuffled)\n",
    "    np.random.shuffle(labels_shuffled)\n",
    "    \n",
    "    train_end = int(n_inputs*train_size)\n",
    "    X_train, X_test = inputs_shuffled[:train_end], inputs_shuffled[train_end:]\n",
    "    Y_train, Y_test = labels_shuffled[:train_end], labels_shuffled[train_end:]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split_numpy(inputs, labels, train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Define model and architecture\n",
    "  \n",
    "Our simple feed-forward neural network will consist of an **input** layer, a single **hidden** layer and an **output** layer. The activation $y$ of each neuron is a weighted sum of inputs, passed through an activation function:  \n",
    "  \n",
    "$$ z = \\sum_{i=1}^n w_i x_i ,$$\n",
    "  \n",
    "$$ y = f(z) ,$$\n",
    "  \n",
    "where $f$ is the activation function, $x_i$ represents input from neuron $i$ in the preceding layer\n",
    "and $w_i$ is the weight to neuron $i$.  \n",
    "The activation of the neurons in the input layer is just the features (e.g. a pixel value).  \n",
    "  \n",
    "The simplest activation function for a binary classifier (e.g. two classes, 0 or 1, cat or not cat)\n",
    "is the **Heaviside** function:\n",
    "  \n",
    "$$ f(z) = \n",
    "\\begin{cases}\n",
    "1,  &  z > 0\\\\\n",
    "0,  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "  \n",
    "A feed-forward neural network with this activation is known as a **perceptron**.  \n",
    "This activation can be generalized to $k$ classes (using e.g. the *one-against-all* strategy), \n",
    "and we call these architectures **multiclass perceptrons**.  \n",
    "  \n",
    "However, it is now common to use the terms Single Layer Perceptron (SLP) (1 hidden layer) and  \n",
    "Multilayer Perceptron (MLP) (2 or more hidden layers) to refer to feed-forward neural networks with any activation function.  \n",
    "  \n",
    "Typical choices for activation functions include the sigmoid function, hyperbolic tangent, and Rectified Linear Unit (ReLU).  \n",
    "We will be using the sigmoid function $\\sigma(x)$:  \n",
    "  \n",
    "$$ f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}} ,$$\n",
    "  \n",
    "which is inspired by probability theory (see logistic regression) and was most commonly used until about 2011.\n",
    "  \n",
    "# Layers\n",
    "  \n",
    "**Input**:  \n",
    "Since each input image has 8x8 = 64 pixels or features, we have an input layer of 64 neurons.  \n",
    "  \n",
    "**Hidden layer**:  \n",
    "We will use 25 neurons in the hidden layer receiving input from the neurons in the input layer.  \n",
    "Since each neuron in the hidden layer is connected to the 64 inputs we have 64x25 = 1600 weights to the hidden layer.  \n",
    "  \n",
    "**Output**:  \n",
    "If we were building a binary classifier, it would be sufficient with a single neuron in the output layer,\n",
    "which could output 0 or 1 according to the Heaviside function. This would be an example of a **hard** classifier, meaning it outputs the class of the input directly. However, if we are dealing with noisy data it is often beneficial to use a **soft** classifier, which outputs the probability of being in class 0 or 1.  \n",
    "  \n",
    "For a soft binary classifier, we could use a single neuron and interpret the output as either being the probability of being in class 0 or the probability of being in class 1. Alternatively we could use 2 neurons, and interpret each neuron as the probability of being in each class.  \n",
    "  \n",
    "Since we are doing multiclass classification, with 10 categories, it is natural to use 10 neurons in the output layer. We number the neurons $j = 0,1,...,9$. The activation of each output neuron $j$ will be according to the **softmax** function:  \n",
    "  \n",
    "$$ P(\\text{class $j$} \\mid \\text{input $\\boldsymbol{x}$}) = \\frac{e^{\\boldsymbol{x}^T \\boldsymbol{w}_j}}\n",
    "{\\sum_{k=0}^{9} e^{\\boldsymbol{x}^T \\boldsymbol{w}_k}} ,$$  \n",
    "  \n",
    "i.e. each neuron $j$ outputs the probability of being in class $j$ given an input from the hidden layer $\\boldsymbol{x}$, with $\\boldsymbol{w}_j$ the weights of neuron $j$ to the inputs.  \n",
    "The denominator is a normalization factor to ensure the outputs sum up to 1.  \n",
    "The exponent is just the weighted sum of inputs as before:  \n",
    "  \n",
    "$$ z_j = \\sum_{i=1}^n w_ {ij} x_i = \\boldsymbol{x}^T \\boldsymbol{w}_j .$$  \n",
    "  \n",
    "Since each neuron in the output layer is connected to the 25 inputs from the hidden layer we have 25x10 = 250\n",
    "weights to the output layer.\n",
    "  \n",
    "# Weights and biases\n",
    "  \n",
    "Typically weights are initialized with small values distributed around zero, drawn from a uniform\n",
    "or normal distribution. Setting all weights to zero means all neurons give the same output, making the network useless.  \n",
    "  \n",
    "Adding a bias value to the weighted sum of inputs allows the neural network to represent a greater range\n",
    "of values. Without it, any input with the value 0 will be mapped to zero. The bias unit has an output of 1, and a weight to each neuron $j$, $b_j$:  \n",
    "  \n",
    "$$ z_j = \\sum_{i=1}^n w_ {ij} x_i + 1\\cdot b_j = \\boldsymbol{x}^T \\boldsymbol{w}_j + b_j .$$  \n",
    "  \n",
    "The bias weights $\\boldsymbol{b}$ are often initialized to zero, but a small value like $0.01$ ensures all neurons have some output which can be backpropagated in the first training cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias](http://ufldl.stanford.edu/tutorial/images/Network331.png)  \n",
    "Via [Stanford UFLDL](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building our neural network\n",
    "\n",
    "n_inputs, n_features = X_train.shape\n",
    "n_hidden_neurons = 25\n",
    "n_categories = 10\n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_hidden_neurons) + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward pass\n",
    "   \n",
    "Since our data has the dimensions $X = (n_{inputs}, n_{features})$ and our weights to the hidden\n",
    "layer have the dimensions  \n",
    "$W_{hidden} = (n_{features}, n_{hidden})$,\n",
    "we can easily feed the network all our training data in one go by taking the dot product  \n",
    "  \n",
    "$$ X^T W = (n_{inputs}, n_{hidden}),$$ \n",
    "  \n",
    "and obtain a matrix that holds the weighted sum of inputs to the hidden layer\n",
    "for each input image.  \n",
    "  \n",
    "For each input image we calculate a weighted sum of inputs (pixels) to each neuron $j$ in the hidden layer:  \n",
    "  \n",
    "$$ z_{j}^{hidden} = \\sum_{i=1}^{n_{features}} w_{ij}^{hidden} x_i + b_{j}^{hidden} = \\boldsymbol{x}^T \\boldsymbol{w}_{j}^{hidden} + b_{j}^{hidden} ,$$\n",
    "  \n",
    "this is then passed through our activation function  \n",
    "  \n",
    "$$ a_{j}^{hidden} = f(z_{j}^{hidden}) .$$  \n",
    "  \n",
    "We calculate a weighted sum of inputs (activations in the hidden layer) to each neuron $j$ in the output layer:  \n",
    "  \n",
    "$$ z_{j}^{output} = \\sum_{i=1}^{n_{hidden}} w_{ij}^{output} a_{i}^{hidden} + b_{j}^{output} = (\\boldsymbol{a}^{hidden})^T \\boldsymbol{w}_{j}^{output} + b_{j}^{output} .$$  \n",
    "  \n",
    "Finally we calculate the output of neuron $j$ in the output layer using the softmax function:  \n",
    "  \n",
    "$$ a_{j}^{output} = \\frac{\\exp{(z_j^{output})}}\n",
    "{\\sum_{k=0}^{9} \\exp{(z_k^{output})}} ,$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities = (n_inputs, n_categories) = (1437, 10)\n",
      "probability that image 0 is in category 0,1,2,...,9 = \n",
      "[1.30129225e-04 1.83087264e-04 2.63322297e-04 2.14710870e-05\n",
      " 9.24959091e-01 3.32452691e-05 6.96952013e-02 4.24138948e-05\n",
      " 3.88303375e-03 7.89004626e-04]\n",
      "probabilities sum up to: 1.000\n",
      "\n",
      "predictions = (n_inputs) = (1437,)\n",
      "prediction for image 0: 4\n",
      "correct label for image 0: 9\n"
     ]
    }
   ],
   "source": [
    "# setup the feed-forward pass\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def feed_forward(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z1 = np.dot(X, hidden_weights)\n",
    "    # activation in the hidden layer\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z2 = np.dot(a1, output_weights)\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z2)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "probabilities = feed_forward(X_train)\n",
    "print(\"probabilities = (n_inputs, n_categories) = \" + str(probabilities.shape))\n",
    "print(\"probability that image 0 is in category 0,1,2,...,9 = \\n\" + str(probabilities[0]))\n",
    "print(\"probabilities sum up to: %.3f\" % probabilities[0].sum())\n",
    "\n",
    "# we obtain a prediction by taking the class with the highest likelihood\n",
    "def predict(X):\n",
    "    probabilities = feed_forward(X)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "predictions = predict(X_train)\n",
    "print()\n",
    "print(\"predictions = (n_inputs) = \" + str(predictions.shape))\n",
    "print(\"prediction for image 0: %d\" % predictions[0])\n",
    "print(\"correct label for image 0: %d\" % Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Choose cost function and optimizer (needs some more work)\n",
    "  \n",
    "To measure how well our neural network is doing we need to introduce a cost function.  \n",
    "We call the function that gives the error of a single output the **loss** function, and the function\n",
    "that gives the total error of our network the **cost** function.\n",
    "A typical choice for multiclass classification is the **cross-entropy** loss, also known as the negative log likelihood.  In multiclass classification it is common to treat each integer label as a so called **one-hot** vector:  \n",
    "  \n",
    "$$ y = 5 \\quad \\rightarrow \\quad \\boldsymbol{y} = (0, 0, 0, 0, 0, 1, 0, 0, 0, 0) ,$$  \n",
    "\n",
    "  \n",
    "$$ y = 1 \\quad \\rightarrow \\quad \\boldsymbol{y} = (0, 1, 0, 0, 0, 0, 0, 0, 0, 0) ,$$  \n",
    "  \n",
    "  \n",
    "i.e. a binary bit string of length $K$, where $K = 10$ is the number of classes.  \n",
    "If $\\boldsymbol{x}_i$ is the $i$-th input (image), $y_{ik}$ refers to the $k$-th component of the $i$-th\n",
    "output vector $\\boldsymbol{y}_i$. The probability of $\\boldsymbol{x}_i$ being in class $k$ is given by the softmax function:  \n",
    "  \n",
    "$$ P(y_{ik} = 1 \\mid \\boldsymbol{x}_i) = \\frac{e^{\\boldsymbol{x}_i^T \\boldsymbol{w}_k}}\n",
    "{\\sum_{k'=0}^{K-1} e^{\\boldsymbol{x}_i^T \\boldsymbol{w}_{k'}}} $$  \n",
    "\n",
    "The probability of not being in class $k$ is just $1 - P(y_{ik} = 1 \\mid \\boldsymbol{x}_i)$.  \n",
    "For Maximum Likelihood Estimation (MLE) we choose the label with the largest probability.  \n",
    "Denote the output label $\\hat{y}$ and the correct label $y$, for example $\\hat{y} = 5$. The likelihood that input $\\boldsymbol{x}_i$\n",
    "gives an output $\\hat{y} = k$ is then\n",
    "  \n",
    "$$ P(\\hat{y} = k \\mid \\boldsymbol{x}_i) = \\prod_{k=0}^{K-1} [P(y_{ik} = 1 \\mid \\boldsymbol{x}_i)]^{y_{im}} \n",
    "\\times [1 - P(y_{im} = 1 \\mid \\boldsymbol{x}_i)]^{1-y_{im}}$$  \n",
    "  \n",
    "A perfect classifier should give a $100 \\%$ probability of the correct label, so the product\n",
    "should just be 1. Otherwise, the likelihood should be smaller.  \n",
    "If we take the log of this we can turn the product into a sum, which is often simpler to compute:  \n",
    "  \n",
    "$$ \\log P(\\hat{y} = k \\mid \\boldsymbol{x}_i) = \\sum_{k=0}^{K-1} y_{ik} \\log P(y_{ik} = 1 \\mid \\boldsymbol{x}_i) \n",
    "+ (1-y_{ik})\\log (1 - P(y_{ik} = 1 \\mid \\boldsymbol{x}_i))$$  \n",
    "  \n",
    "For a perfect classifier this should just be $\\log 1 = 0$. Otherwise we get a negative number.  \n",
    "Since it is easier to think in terms of minimizing a positive number, we take our loss function\n",
    "to be the negative log-likelihood:  \n",
    "  \n",
    "$$ \\mathcal{L}(\\boldsymbol{w}) = - \\log P(\\hat{y} = k \\mid \\boldsymbol{x}_i) $$  \n",
    "  \n",
    "We then take the average of the loss function over all input samples to define the cost function:  \n",
    "$$ \\begin{split} \\mathcal{C}(\\boldsymbol{w}) &= \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(\\boldsymbol{w}) \\\\\n",
    " &= -\\frac{1}{N}\\sum_{i=1}^N \\sum_{k=0}^{K-1} y_{ik} \\log P(y_{ik} = 1 \\mid \\boldsymbol{x}_i) \n",
    "+ (1-y_{ik})\\log (1 - P(y_{ik} = 1 \\mid \\boldsymbol{x}_i)) \\end{split} .$$  \n",
    "  \n",
    "# Optimizing the cost function\n",
    "  \n",
    "Text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Train the model\n",
    "  \n",
    "Backpropagation equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old predictions for images 0-5: [4 4 9 4 4]\n",
      "correct labels for images 0-5: [9 2 9 2 6]\n",
      "\n",
      "new predictions for images 0-5: [0 0 0 0 0]\n",
      "correct labels for images 0-5: [9 2 9 2 6]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train_onehot, Y_test_onehot = to_categorical(Y_train), to_categorical(Y_test)\n",
    "\n",
    "def feed_forward_train(X):\n",
    "    # weighted sum of inputs to the hidden layer\n",
    "    z1 = np.dot(X, hidden_weights)\n",
    "    # activation in the hidden layer\n",
    "    a1 = sigmoid(z1)\n",
    "    \n",
    "    # weighted sum of inputs to the output layer\n",
    "    z2 = np.dot(a1, output_weights)\n",
    "    # softmax output\n",
    "    # axis 0 holds each input and axis 1 the probabilities of each category\n",
    "    exp_term = np.exp(z2)\n",
    "    probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "    \n",
    "    return a1, probabilities\n",
    "\n",
    "def backpropagation(X, Y):\n",
    "    a1, probabilities = feed_forward_train(X)\n",
    "    \n",
    "    # error in the output layer\n",
    "    error_output = Y - probabilities\n",
    "    # error in the hidden layer\n",
    "    error_hidden = np.dot(error_output, output_weights.T) * a1 * (1 - a1)\n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.dot(a1.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.dot(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient\n",
    "\n",
    "predictions = predict(X_train)\n",
    "print(\"old predictions for images 0-5: \" + str(predictions[0:5]))\n",
    "print(\"correct labels for images 0-5: \" + str(Y_train[0:5]))\n",
    "print()\n",
    "\n",
    "eta = 0.1\n",
    "for i in range(100):\n",
    "    dWo, dBo, dWh, dBh = backpropagation(X_train, Y_train_onehot)\n",
    "    \n",
    "    output_weights -= eta * dWo\n",
    "    output_bias -= eta * dBo\n",
    "    hidden_weights -= eta * dWh\n",
    "    hidden_bias -= eta * dBh\n",
    "\n",
    "predictions = predict(X_train)\n",
    "print(\"new predictions for images 0-5: \" + str(predictions[0:5]))\n",
    "print(\"correct labels for images 0-5: \" + str(Y_train[0:5]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full object-oriented implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Evaluate model performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Adjust hyperparameters (if necessary, network architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving the backpropagation equations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
