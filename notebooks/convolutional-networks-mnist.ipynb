{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()\n",
    "    \n",
    "mnist_data = digits.images\n",
    "mnist_data = mnist_data[:,np.newaxis,:,:]\n",
    "labels = digits.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork:\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.n_inputs, self.depth, self.input_width, self.input_height = X_data.shape\n",
    "        \n",
    "        self.eta = 0.1\n",
    "        \n",
    "        self.padding = 1\n",
    "        self.receptive_field = 3\n",
    "        self.stride = 1\n",
    "        self.downsampling = 2\n",
    "        \n",
    "        self.n_filters_conv = 3\n",
    "        self.n_neurons_flattened = int(self.n_filters_conv*self.input_width*self.input_height)\n",
    "        self.n_neurons_connected = 10\n",
    "        self.n_categories = 10\n",
    "        \n",
    "        self.output_width = int((self.input_width - self.receptive_field + 2 * self.padding) / self.stride + 1)\n",
    "        self.output_height = int((self.input_height - self.receptive_field + 2 * self.padding) / self.stride + 1)\n",
    "        self.downsampling_width = int(self.output_width / self.downsampling)\n",
    "        self.downsampling_height = int(self.output_height / self.downsampling)\n",
    "        \n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.weights_conv = np.random.randn(self.n_filters_conv, self.depth, self.receptive_field, self.receptive_field)\n",
    "        self.bias_conv = np.zeros(self.n_filters_conv)\n",
    "        \n",
    "        self.weights_connected = np.random.randn(self.n_neurons_flattened, self.n_neurons_connected)\n",
    "        self.bias_connected = np.zeros(self.n_neurons_connected)\n",
    "        \n",
    "        self.weights_output = np.random.randn(self.n_neurons_connected, self.n_categories)\n",
    "        self.bias_output = np.zeros(self.n_categories)\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        # Convolution layer\n",
    "        self.z1 = np.zeros((self.n_inputs, self.n_filters_conv, self.output_width, self.output_height))\n",
    "        for n in range(self.n_inputs):\n",
    "            X_padded = np.pad(self.X_data[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = w*self.stride\n",
    "                        h2 = w*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.z1[n, f, w, h] = np.sum(matrix_slice*self.weights_conv[f,:,:,:])\n",
    "        self.a1 = np.maximum(self.z1, 0)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        self.z2 = np.zeros_like(self.a1)\n",
    "        for n in range(self.n_inputs):\n",
    "            for w in range(self.downsampling_width):\n",
    "                for h in range(self.downsampling_height):\n",
    "                    w1 = w*self.downsampling\n",
    "                    w2 = w*self.downsampling + self.downsampling\n",
    "                    h1 = h*self.downsampling\n",
    "                    h2 = h*self.downsampling + self.downsampling\n",
    "                    matrix_slice = self.a1[n,:,w1:w2,h1:h2]\n",
    "                    matrix_slice[matrix_slice != matrix_slice.max()] = 0\n",
    "                    self.z2[n,:,w1:w2,h1:h2] = matrix_slice\n",
    "        self.a2 = np.maximum(self.z2, 0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.downsampled_array = self.a2.reshape(-1, self.n_neurons_flattened)\n",
    "        self.z3 = np.dot(self.downsampled_array, self.weights_connected) + self.bias_connected\n",
    "        self.a3 = np.maximum(self.z3, 0)\n",
    "        \n",
    "        # Output layer\n",
    "        exp_term = np.exp(self.a3)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        self.probabilities[np.abs(self.probabilities) < 1E-16] = 0\n",
    "        \n",
    "    def feed_forward_out(self, X):\n",
    "        # Convolution layer\n",
    "        n_inputs = X.shape[0]\n",
    "        z1 = np.zeros((n_inputs, self.n_filters_conv, self.output_width, self.output_height))\n",
    "        for n in range(n_inputs):\n",
    "            X_padded = np.pad(X[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = w*self.stride\n",
    "                        h2 = w*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.z1[n, f, w, h] = np.sum(matrix_slice*self.weights_conv[f,:,:,:])\n",
    "        a1 = np.maximum(z1, 0)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        z2 = np.zeros_like(a1)\n",
    "        for n in range(n_inputs):\n",
    "            for w in range(self.downsampling_width):\n",
    "                for h in range(self.downsampling_height):\n",
    "                    w1 = w*self.downsampling\n",
    "                    w2 = w*self.downsampling + self.downsampling\n",
    "                    h1 = h*self.downsampling\n",
    "                    h2 = h*self.downsampling + self.downsampling\n",
    "                    matrix_slice = a1[n,:,w1:w2,h1:h2]\n",
    "                    matrix_slice[matrix_slice != matrix_slice.max()] = 0\n",
    "                    z2[n,:,w1:w2,h1:h2] = matrix_slice\n",
    "        a2 = np.maximum(self.z2, 0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        downsampled_array = a2.reshape(-1, self.n_neurons_flattened)\n",
    "        z3 = np.dot(downsampled_array, self.weights_connected) + self.bias_connected\n",
    "        a3 = np.maximum(z3, 0)\n",
    "        \n",
    "        # Output layer\n",
    "        exp_term = np.exp(a3)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        probabilities[np.abs(probabilities) < 1E-16] = 0\n",
    "        return probabilities\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        # Output layer\n",
    "        error_output = self.probabilities\n",
    "        error_output[range(self.n_inputs), self.Y_data] -= 1\n",
    "        self.weights_output_gradient = np.dot(self.a3.T, error_output)\n",
    "        self.bias_output_gradient = np.sum(error_output)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        error_connected = np.dot(error_output, self.weights_output.T)*(self.a3 > 0)\n",
    "        self.weights_connected_gradient = np.dot(self.downsampled_array.T, error_connected)\n",
    "        self.bias_connected_gradient = np.sum(error_connected)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        error_downsampling = np.dot(error_connected, self.weights_connected.T)*(self.downsampled_array > 0)\n",
    "        error_downsampling = error_downsampling.reshape(-1, self.n_filters_conv, self.output_width, self.output_height)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        error_conv = error_downsampling\n",
    "        self.weights_conv_gradient = np.zeros_like(self.weights_conv)\n",
    "        for n in range(self.n_inputs):\n",
    "            X_padded = np.pad(self.X_data[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = w*self.stride\n",
    "                        h2 = w*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.weights_conv_gradient[f,:,:,:] += X_padded[:,w1:w2,h1:h2]*error_conv[n, f, w, h]\n",
    "        self.bias_conv_gradient = np.sum(error_conv)\n",
    "        \n",
    "        self.weights_output -= self.eta * self.weights_output_gradient\n",
    "        self.bias_output -= self.eta * self.bias_output_gradient\n",
    "        self.weights_connected -= self.eta * self.weights_connected_gradient\n",
    "        self.bias_connected -= self.eta * self.bias_connected_gradient\n",
    "        self.weights_conv -= self.eta * self.weights_conv_gradient\n",
    "        self.bias_conv -= self.eta * self.bias_conv_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cnn = ConvolutionalNeuralNetwork(X_train, Y_train)\n",
    "for i in tqdm(range(10)):\n",
    "    cnn.feed_forward()\n",
    "    cnn.backpropagation()\n",
    "\n",
    "pred = cnn.feed_forward_out(X_test)\n",
    "print(pred[0].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width,\n",
    "        input_height,\n",
    "        n_filters=10,\n",
    "        n_neurons_connected=50,\n",
    "        n_categories=10,\n",
    "        depth=1,\n",
    "        receptive_field=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.n_downsampled = int(input_width*input_height*n_filters / 4)\n",
    "        self.n_neurons_connected = n_neurons_connected\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.receptive_field = receptive_field\n",
    "        self.stride = stride\n",
    "        self.strides = [self.stride, self.stride, self.stride, self.stride]\n",
    "        self.padding = padding\n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_CNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.input_width, self.input_height, self.depth), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_CNN(self):\n",
    "        with tf.name_scope('CNN'):\n",
    "            \n",
    "            # Convolutional layer\n",
    "            W_conv = self.weight_variable([self.receptive_field, self.receptive_field, self.depth, self.n_filters], name='conv', dtype=tf.float32)\n",
    "            b_conv = self.weight_variable([self.n_filters], name='conv', dtype=tf.float32)\n",
    "            z_conv = tf.nn.conv2d(self.X, W_conv, self.strides, padding='SAME', name='conv') + b_conv\n",
    "            a_conv = tf.nn.relu(z_conv)\n",
    "            \n",
    "            # 2x2 max pooling\n",
    "            a_pool = tf.nn.max_pool(a_conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            \n",
    "            # Fully connected layer\n",
    "            a_pool_flat = tf.reshape(a_pool, [-1, self.n_downsampled])\n",
    "            W_fc = self.weight_variable([self.n_downsampled, self.n_neurons_connected], name='fc', dtype=tf.float32)\n",
    "            b_fc = self.bias_variable([self.n_neurons_connected], name='fc', dtype=tf.float32)\n",
    "            a_fc = tf.nn.relu(tf.matmul(a_pool_flat, W_fc) + b_fc)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_connected, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "mnist_data = mnist_data[:,0,:,:]\n",
    "mnist_data = mnist_data[:,:,:,np.newaxis]\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-726ef61e97ec>:71: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Train accuracy: 1.000\n",
      "Test accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "CNN = CNNModel(8, 8)\n",
    "\n",
    "n_inputs = X_train.shape[0]\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "iterations = n_inputs // batch_size\n",
    "data_indices = np.arange(n_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([CNN.loss, CNN.optimizer],\n",
    "                     feed_dict={CNN.X: batch_X,\n",
    "                                CNN.Y: batch_Y})\n",
    "            accuracy = sess.run(CNN.accuracy,\n",
    "                                feed_dict={CNN.X: batch_X,\n",
    "                                           CNN.Y: batch_Y})\n",
    "            step = sess.run(CNN.global_step)\n",
    "    \n",
    "    train_loss, train_accuracy = sess.run([CNN.loss, CNN.accuracy],\n",
    "                                          feed_dict={CNN.X: X_train,\n",
    "                                                     CNN.Y: Y_train})\n",
    "    \n",
    "    test_loss, test_accuracy = sess.run([CNN.loss, CNN.accuracy],\n",
    "                                        feed_dict={CNN.X: X_test,\n",
    "                                                   CNN.Y: Y_test})\n",
    "    \n",
    "    print(\"Train accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %.3f\" % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
