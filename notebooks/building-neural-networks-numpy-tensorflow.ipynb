{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "Artificial neural networks are computational systems that can learn to perform tasks by considering examples,\n",
    "generally without being programmed with any task-specific rules. It is supposed to mimic a biological system, wherein neurons interact by sending signals in the form of mathematical functions between layers. All layers can contain an arbitrary number of neurons, and each connection is represented by a weight variable.  \n",
    "  \n",
    "In this tutorial we will build a feed-forward neural network, where information moves in only in direction:\n",
    "forward through the layers. Each neuron or *node* is represented by a circle, while arrows display the connections\n",
    "between the nodes and indicate the direction of information flow. Each node in a layer is connected to all nodes in the subsequent layer, which makes this a so-called *fully-connected* feed-forward neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FFNN](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)  \n",
    "Via [Wikipedia](https://en.wikipedia.org/wiki/File:Colored_neural_network.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "To follow this tutorial we require an installation of Python with the numerical package **numpy**, either:  \n",
    "1) Python 2.7.x  \n",
    "2) Python 3.5.x or greater  \n",
    "  \n",
    "With a version of Numpy 1.0.x or greater.  \n",
    "We will also use the packages **matplotlib**, **scikit-learn**, **Tensorflow** and **Keras**, though these are not strictly necessary.  \n",
    "To open and run this notebook you also need an installation of **IPython** and **Jupyter Notebook**.  \n",
    "  \n",
    "# Anaconda\n",
    "Anaconda is a free and open source Python and R distribution, that aims to simplify package management and deployment. Anaconda comes with more than 1000 data packages, as well as the Conda package and package and virtual environment manager. Anaconda is available on Linux, OS X and Windows systems, and contains nearly all prerequisite software, and comes highly recommended.  \n",
    "If Anaconda is installed you can install Tensorflow and Keras using:  \n",
    "  \n",
    "```conda install tensorflow```  \n",
    "```conda install keras```  \n",
    "  \n",
    "(You may run into minor problems with conflicting package versions)  \n",
    "  \n",
    "# Pip package manager\n",
    "If you do not wish to install Anaconda you may download Python from [here](https://www.python.org/downloads/),\n",
    "or you can use package managers like **brew**, **apt**, **pacman**,...  \n",
    "Python distributions come with their own package manager, **pip**, and once you have Python installed\n",
    "you can run the following command:  \n",
    "  \n",
    "```pip install numpy matplotlib scikit-learn ipython jupyter```  \n",
    "  \n",
    "To install Tensorflow follow the instructions [here](https://www.tensorflow.org/install/).  \n",
    "After you have installed tensorflow you can install keras:  \n",
    "  \n",
    "```pip install keras```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python](https://imgs.xkcd.com/comics/python_environment.png)  \n",
    "Via [xkcd](https://xkcd.com/1987/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow\n",
    "  \n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "  \n",
    "**\n",
    "1) Collect and pre-process data  \n",
    "2) Define model and architecture  \n",
    "3) Choose cost function and optimizer  \n",
    "4) Train the model  \n",
    "5) Evaluate model performance on test data  \n",
    "6) Adjust hyperparameters (if necessary, network architecture)\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Collect and pre-process data\n",
    "  \n",
    "In this tutorial we will be using the MNIST dataset, which is readily available through the **scikit-learn**\n",
    "package. You may also find it for example [here](http://yann.lecun.com/exdb/mnist/).  \n",
    "The **MNIST** (Modified National Institute of Standards and Technology) database is a large database\n",
    "of handwritten digits that is commonly used for training various image processing systems.  \n",
    "The MNIST dataset consists of 70 000 images of size 28x28 pixels, each labeled from 0 to 9.  \n",
    "  \n",
    "To feed data into a feed-forward neural network we need to represent the inputs as a feature matrix $X = [n_{inputs}, \n",
    "n_{features}]$.  \n",
    "Each row represents an **input**, in this case a handwritten digit, and each column represents a **feature**, in this case a pixel.  \n",
    "The correct answers, also known as **labels** or **targets** are represented as a 1D array of integers $Y = [5, 3, 1, 8,...]$.  \n",
    "  \n",
    "Say I wanted to build a neural network using supervised learning to predict Body-Mass Index (BMI) from\n",
    "measurements of height (in m)  \n",
    "and weight (in kg). If I had measurements of 5 people the feature matrix could be for example:  \n",
    "  \n",
    "$$ X = \\begin{bmatrix}\n",
    "1.85 & 81\\\\\n",
    "1.71 & 65\\\\\n",
    "1.95 & 103\\\\\n",
    "1.55 & 42\\\\\n",
    "1.63 & 56\n",
    "\\end{bmatrix} ,$$  \n",
    "  \n",
    "and the targets would be:  \n",
    "  \n",
    "$$ Y = (23.7, 22.2, 27.1, 17.5, 21.1) $$  \n",
    "  \n",
    "Since each input image is a 2D matrix, we need to flatten the image (i.e. \"unravel\" the 2D matrix into a 1D array)  \n",
    "to turn the data into a feature matrix. This means we lose all spatial information in the image, such as locality and translational invariance ([explanation](https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo))  \n",
    "More complicated architectures such as Convolutional Neural Networks can take advantage\n",
    "of such information, and is most commonly applied when analyzing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = (n_inputs, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "labels = (n_inputs) = (1797,)\n",
      "X = (n_inputs, n_features) = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAACPCAYAAADnRe/OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACqRJREFUeJzt3V2IZOlZB/D/k92sMWi2kyjxI+7OrhdqJPQMCi4o7EAWYgTduTBXBjOj8VZ30fiFIesHIkGcHhE/LtTB9UZRtwfURIVsj4pfNz2zEkXBbC8RGUlke8yGyJr4elE12A6TzfvWdHWd6v794EDV8JxT55ynT/W/T9f0U621AADwyl616h0AAFgHQhMAQAehCQCgg9AEANBBaAIA6CA0AQB0OHGhqap2quo9R70uh08vjxf9PD708vjQy/9vbUNTVe1V1WOr3o/PparOV9Vnq+qlA8vZVe/XFE29lwdV1YerqlXVvavel6lah35W1ZNVdaOqblbVb1TVF6x6n6ZoHXp5i2vzla1DL6vq4ar6w6r6ZFV9oqo+sOp9ut3ahqY18dettS86sOyseodYXFV9VxJvyGuuqt6e5EeTvC3JqSQPJ/nJVe4Td8e1uf6q6r4kf5bkw0m+LMmbk/z2SnfqDo5daKqq18+T6ser6sX54zffVvbVVfV3858yr1TVGw6s/0hV/VVV7VfVdXeHVmdKvayq+5O8P8kPL7qNk25C/Xx3kl9vrX2ktfZikp9Ocn7BbZ1IE+qla/MuTaiX55P8W2vtF1prn2qt/Vdr7bkFt7U0xy40ZXZMv5nkwSQPJPl0kl+6rea7k3xPkq9I8pkkv5gkVfWVSf4oyc8keUOSH0ry+1X1pbe/SFU9MP8ieeAV9uXM/BbjP1fV+9w2HjalXv5skl9JcuNuDuiEm0o/vz7J9QPPryd5U1W9ccHjOomm0svEtXm3ptLLR5LsVdUH5983d6rqrXd9dIettbaWS5K9JI911J1O8uKB5ztJfu7A87ckeTnJPUl+JMnTt63/J0nefWDd93Tu38NJHsrsC/KtSf4hyY+t+rxNcVmDXn5jkmuZ3f4/laQluXfV522qyxr081+SfOuB56+e9/TUqs/d1JY16KVr8/j08k+T/HeSdyS5L8l7k3w0yX2rPncHl2N3p6mqXltVv1ZVL1TVfyb58yQbVXXPgbKPHXj8QmZvml+SWdJ+5zwN71fVfpJvSfLlo/vRWvtoa+351tr/tNb+PslPJfnORY/rJJpCL6vqVUl+OckPtNY+czfHc9JNoZ9zLyV53YHntx5/coFtnUhT6KVr83BMoZdzn07yl621D7bWXk7y80nemOTrFtjW0hzHXxf9YJKvSfJNrbUbVXU6yW6SOlDzVQceP5BZuv1EZl8YT7fWvm8J+9Vu2wc+vyn08nWZ/TT7O1WVzH66SpJ/rap3ttb+4i63f5JMoZ9J8pEkm0l+d/58M8m/t9b+4xC2fVJMoZeuzcMxhV4myXNJvvkQtrNU636n6dVV9ZoDy71JvjizxLo//7Da+++w3ruq6i1V9drM7gD9Xmvts5l9Uv/bq+rtVXXPfJtn7/ChuM+rqt5RVW+aP/7aJO9LcmXB4zwJptrLm5n9Hv/0fPm2+b9/Q5K/HT/ME2Oq/UyS30ryvfPXeX2Sn0hyeZGDPCGm2kvX5rip9jLzbT1SVY/N73I9kVkw+8dFDnRZ1j00/XFmzb61PJVkK8kXZnay/ybJh+6w3tOZvUneSPKaJN+fJK21jyV5PMmPJ/l4Zin6vbnDearZh9peqs/9oba3JXmuqj41388/yOwDi9zZJHvZZm7cWubbSmZ3Jl5e9GBPgEn2c76tDyX5QJJnM/tVwwu58zcKZibZS9fmQibZy/m2/inJu5L8apIX59v9jqn1suYfwAIA4BWs+50mAIAjITQBAHQQmgAAOghNAAAdhCYAgA7L+uOWQ/8lb29vb2jjDz300FD9FO3u7g7Vnz59evQlDusPaU7qv1fu7+8P1Z8/f374Nc6ePTtU/8QTTwy/xqC16OXOzs5Q/eh529jYGKpPku3t7aW/xqDD/AO3k7o2R/s/2psk2draGl5nydbi2hw1em2Ovi8nyeXLl4fXWbKuXrrTBADQQWgCAOggNAEAdBCaAAA6CE0AAB2EJgCADkITAEAHoQkAoIPQBADQQWgCAOiwrDEqQ5Y9umBzc3OofpGxGKdOnRqqX2AsChkfi3LlypXh17hw4cLwOsfR6Hijc+fODdWPXvfXr18fqk/GR+Jcu3Zt+DWYeeqpp4bqj2BkDXOj42cuXbo0VH/x4sWh+nXmThMAQAehCQCgg9AEANBBaAIA6CA0AQB0EJoAADoITQAAHYQmAIAOQhMAQAehCQCgg9AEANBhErPnRmcWjbp8+fJQvblwR2d0zt/Ozs5Q/f333z9UnyTPP//88DrH0ei5vnnz5lD99vb2UP3oLLxkfI7g6KzCxx9/fKh+nYz2f39/f6j+6tWrQ/XJ+D6Nzh48rka/B47Oax2dCbrO3GkCAOggNAEAdBCaAAA6CE0AAB2EJgCADkITAEAHoQkAoIPQBADQQWgCAOggNAEAdBCaAAA6TGL23LKNzsVZZI7O6Ay142p0dtelS5eG6nd3d4fqz5w5M1SfmD14y+ist9E5f6PneZE5YqMzt0a/vo7z7LnRmaCj74HLnjnK/xm9lkd7s7GxMVS/ztxpAgDoIDQBAHQQmgAAOghNAAAdhCYAgA5CEwBAB6EJAKCD0AQA0EFoAgDoIDQBAHQQmgAAOkxi9tzorLfROTqj9U8++eRQ/SKvsbW1Nfwa6+CZZ54Zqt/c3ByqH50ltohlzyo8rjO3RmfJnaR5VVOws7MzVH/16tWh+tH+7+/vD9Un4/PtRt8v1mXu5Ggvb968OVQ/+h61vb09VJ8k586dG6qfynxXd5oAADoITQAAHYQmAIAOQhMAQAehCQCgg9AEANBBaAIA6CA0AQB0EJoAADoITQAAHYQmAIAOQhMAQIdJDOwdHZK4yHDAEYsM0x0d8js6EHFdhpuODlU8e/bsUP2lS5eG6hcxeq5PnTq1nB1ZsTNnzgzVL/u6XGTA6yLrMPPoo48O1V+7dm2ofnSIbDLez9F9WpeBvct+zxk9D4sMIR/9Pjvay2UNd3enCQCgg9AEANBBaAIA6CA0AQB0EJoAADoITQAAHYQmAIAOQhMAQAehCQCgg9AEANBBaAIA6FCttWVsd2ije3t7QxsfnW924cKFofqLFy8O1SfjxzBav4A6pO0s5QvkqFSNn4bd3d2h+iOYV7WSXo7O+Rqdh3Xu3Lmh+kWumatXrw7VH0HvD6uXyQm8Np999tmh+tHZlgtYi/fZ0dlwo3PbFpkJOtrL0dmWC7xfdPXSnSYAgA5CEwBAB6EJAKCD0AQA0EFoAgDoIDQBAHQQmgAAOghNAAAdhCYAgA5CEwBAB6EJAKDDvavegWR8xtWVK1eWWr+I0bk4HI0HH3xweJ2NjY0l7Mn6GT0PW1tbQ/WjMyE3NzeH6pPx+VZHMEfw2DqCeZquzQWNzp4bnSN5FPNaR2dVLos7TQAAHYQmAIAOQhMAQAehCQCgg9AEANBBaAIA6CA0AQB0EJoAADoITQAAHYQmAIAOQhMAQIdqra16HwAAJs+dJgCADkITAEAHoQkAoIPQBADQQWgCAOggNAEAdBCaAAA6CE0AAB2EJgCADkITAEAHoQkAoIPQBADQQWgCAOggNAEAdBCaAAA6CE0AAB2EJgCADkITAEAHoQkAoIPQBADQQWgCAOggNAEAdBCaAAA6/C/Odja+DeIlxgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# display images in notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(\"inputs = (n_inputs, pixel_width, pixel_height) = \" + str(inputs.shape))\n",
    "print(\"labels = (n_inputs) = \" + str(labels.shape))\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(\"X = (n_inputs, n_features) = \" + str(inputs.shape))\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[random_indices[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and test datasets\n",
    "\n",
    "Performing analysis before partitioning the dataset is a major error, that can lead to incorrect conclusions  \n",
    "(see \"Bias-Variance Tradeoff\", for example [here](https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/)).  \n",
    "  \n",
    "We will reserve $80 \\%$ of our dataset for training and $20 \\%$ for testing.  \n",
    "  \n",
    "It is important that the train and test datasets are drawn randomly from our dataset, to ensure\n",
    "no bias in the sampling.  \n",
    "Say you are taking measurements of weather data to predict the weather in the coming 5 days.\n",
    "You don't want to train your model on measurements taken from the hours 00.00 to 12.00, and then test it on data\n",
    "collected from 12.00 to 24.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# one-liner from scikit-learn library\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "# equivalently in numpy\n",
    "def train_test_split_numpy(inputs, labels, train_size, test_size):\n",
    "    n_inputs = len(inputs)\n",
    "    inputs_shuffled = inputs.copy()\n",
    "    labels_shuffled = labels.copy()\n",
    "    \n",
    "    np.random.shuffle(inputs_shuffled)\n",
    "    np.random.shuffle(labels_shuffled)\n",
    "    \n",
    "    train_end = int(n_inputs*(1-test_size))\n",
    "    X_train, X_test = inputs_shuffled[:train_end], inputs_shuffled[train_end:]\n",
    "    Y_train, Y_test = labels_shuffled[:train_end], labels_shuffled[train_end:]\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split_numpy(inputs, labels, train_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Define model and architecture\n",
    "  \n",
    "Our simple feed-forward neural network will consist of an **input** layer, a single **hidden** layer and an **output** layer. The activation $y$ of each neuron is a weighted sum of inputs, passed through an activation function:  \n",
    "  \n",
    "$$ z = \\sum_{i=1}^n w_i x_i ,$$\n",
    "  \n",
    "$$ y = f(z) ,$$\n",
    "  \n",
    "where $f$ is the activation function, $x_i$ represents input from neuron $i$ in the preceding layer\n",
    "and $w_i$ is the weight to neuron $i$.  \n",
    "The activation of the neurons in the input layer is just the features (e.g. a pixel value).  \n",
    "  \n",
    "The simplest activation function for a binary classifier (e.g. two classes, 0 or 1, cat or not cat)\n",
    "is the **Heaviside** function:\n",
    "  \n",
    "$$ f(z) = \n",
    "\\begin{cases}\n",
    "1,  &  z > 0\\\\\n",
    "0,  & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "  \n",
    "A feed-forward neural network with this activation is known as a **perceptron**.  \n",
    "This activation can be generalized to $k$ classes (using e.g. the *one-against-all* strategy), \n",
    "and we call these architectures **multiclass perceptrons**.  \n",
    "  \n",
    "However, it is now common to use the terms Single Layer Perceptron (SLP) (1 hidden layer) and  \n",
    "Multilayer Perceptron (MLP) (2 or more hidden layers) to refer to feed-forward neural networks with any activation function.\n",
    "\n",
    "**Input**: Since each input image has 8x8 = 64 pixels or features, we have an input layer of 64 neurons.  \n",
    "**Hidden layer**: We will use 25 neurons in the hidden layer receiving input from the neurons in the input layer.\n",
    "Since each neuron is connected to the 64 inputs we have 64x25 = 1600 weights to the hidden layer.  \n",
    "**Output**: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
