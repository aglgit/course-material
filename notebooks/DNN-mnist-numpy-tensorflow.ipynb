{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "    \n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "mnist_data = digits.images.reshape((n_samples, -1)).astype(float)\n",
    "labels = digits.target.astype(int)\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 100\n",
    "n_features = X_train.shape[1]\n",
    "n_neurons_layer1 = 100\n",
    "n_neurons_layer2 = 50\n",
    "n_categories = 10\n",
    "\n",
    "eta_vals = np.logspace(-5, 0, 6)\n",
    "lmbd_vals = np.logspace(-5, 0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from tqdm import tqdm\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_data,\n",
    "        Y_data,\n",
    "        epochs=1,\n",
    "        batch_size=100,\n",
    "        eta=1e-2,\n",
    "        lmbd=0.0,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "    ):\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.weights_layer1 = np.random.randn(self.n_features, self.n_neurons_layer1)\n",
    "        self.bias_layer1 = np.zeros(self.n_neurons_layer1)\n",
    "\n",
    "        self.weights_layer2 = np.random.randn(self.n_neurons_layer1, self.n_neurons_layer2)\n",
    "        self.bias_layer2 = np.zeros(self.n_neurons_layer2)\n",
    "\n",
    "        self.weights_output = np.random.randn(self.n_neurons_layer2, self.n_categories)\n",
    "        self.bias_output = np.zeros(self.n_categories)\n",
    "\n",
    "    def feed_forward(self):\n",
    "        self.z1 = np.dot(self.X_data, self.weights_layer1) + self.bias_layer1\n",
    "        self.a1 = expit(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights_layer2) + self.bias_layer2\n",
    "        self.a2 = expit(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.weights_output) + self.bias_output\n",
    "\n",
    "        exp_term = np.exp(self.z3)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "        z1 = np.dot(X, self.weights_layer1) + self.bias_layer1\n",
    "        a1 = expit(z1)\n",
    "\n",
    "        z2 = np.dot(a1, self.weights_layer2) + self.bias_layer2\n",
    "        a2 = expit(z2)\n",
    "\n",
    "        z3 = np.dot(a2, self.weights_output) + self.bias_output\n",
    "\n",
    "        exp_term = np.exp(z3)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backpropagation(self):\n",
    "        error_output = self.probabilities\n",
    "        error_output[range(self.n_inputs), self.Y_data] -= 1\n",
    "        error_layer2 = (np.dot(error_output, self.weights_output.T) * self.a2 * (1 - self.a2))\n",
    "        error_layer1 = (np.dot(error_layer2, self.weights_layer2.T) * self.a1 * (1 - self.a1))\n",
    "\n",
    "        self.weights_output_gradient = np.dot(self.a2.T, error_output)\n",
    "        self.bias_output_gradient = np.sum(error_output)\n",
    "\n",
    "        self.weights_layer2_gradient = np.dot(self.a1.T, error_layer2)\n",
    "        self.bias_layer2_gradient = np.sum(error_layer2)\n",
    "\n",
    "        self.weights_layer1_gradient = np.dot(self.X_data.T, error_layer1)\n",
    "        self.bias_layer1_gradient = np.sum(error_layer1)\n",
    "\n",
    "        if self.lmbd > 0.0:\n",
    "            self.weights_output_gradient += self.lmbd * self.weights_output\n",
    "            self.weights_layer2_gradient += self.lmbd * self.weights_layer2\n",
    "            self.weights_layer1_gradient += self.lmbd * self.weights_layer1\n",
    "\n",
    "        self.weights_output -= self.eta * self.weights_output_gradient\n",
    "        self.bias_output -= self.eta * self.bias_output_gradient\n",
    "        self.weights_layer2 -= self.eta * self.weights_layer2_gradient\n",
    "        self.bias_layer2 -= self.eta * self.bias_layer2_gradient\n",
    "        self.weights_layer1 -= self.eta * self.weights_layer1_gradient\n",
    "        self.bias_layer1 -= self.eta * self.bias_layer1_gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.n_inputs = self.X_data.shape[0]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = NeuralNetwork(X_train, Y_train, eta=eta, lmbd=lmbd, epochs=10, batch_size=batch_size,\n",
    "                                  n_neurons_layer1=n_neurons_layer1, n_neurons_layer2=n_neurons_layer2,\n",
    "                                  n_categories=n_categories)\n",
    "        dnn.train()\n",
    "        \n",
    "        DNN_numpy[i][j] = dnn\n",
    "        \n",
    "        test_predict = dnn.predict(X_test)\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_neurons_layer1, n_neurons_layer2), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=100)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        \n",
    "        DNN_scikit[i][j] = dnn\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", dnn.score(X_test, Y_test))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_DNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.n_features), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_DNN(self):\n",
    "        with tf.name_scope('DNN'):\n",
    "            \n",
    "            # Fully connected layer 1\n",
    "            W_fc1 = self.weight_variable([self.n_features, self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            b_fc1 = self.bias_variable([self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            a_fc1 = tf.nn.sigmoid(tf.matmul(self.X, W_fc1) + b_fc1)\n",
    "            \n",
    "            # Fully connected layer 2\n",
    "            W_fc2 = self.weight_variable([self.n_neurons_layer1, self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            b_fc2 = self.bias_variable([self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            a_fc2 = tf.nn.sigmoid(tf.matmul(a_fc1, W_fc2) + b_fc2)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_layer2, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc2, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.995\n",
      "Test accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "DNN = DNNModel(n_features, n_neurons_layer1, n_neurons_layer2, n_categories, eta=0.1)\n",
    "\n",
    "n_inputs = X_train.shape[0]\n",
    "iterations = n_inputs // batch_size\n",
    "data_indices = np.arange(n_inputs)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([DNN.loss, DNN.optimizer],\n",
    "                     feed_dict={DNN.X: batch_X,\n",
    "                                DNN.Y: batch_Y})\n",
    "            accuracy = sess.run(DNN.accuracy,\n",
    "                                feed_dict={DNN.X: batch_X,\n",
    "                                           DNN.Y: batch_Y})\n",
    "            step = sess.run(DNN.global_step)\n",
    "    \n",
    "    train_loss, train_accuracy = sess.run([DNN.loss, DNN.accuracy],\n",
    "                                          feed_dict={DNN.X: X_train,\n",
    "                                                     DNN.Y: Y_train})\n",
    "    \n",
    "    test_loss, test_accuracy = sess.run([DNN.loss, DNN.accuracy],\n",
    "                                        feed_dict={DNN.X: X_test,\n",
    "                                                   DNN.Y: Y_test})\n",
    "    \n",
    "    print(\"Train accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %.3f\" % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
