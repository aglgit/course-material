{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "    \n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "mnist_data = digits.images.reshape((n_samples, -1)).astype(float)\n",
    "labels = digits.target.astype(int)\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "n_features = X_train.shape[1]\n",
    "n_neurons_layer1 = 100\n",
    "n_neurons_layer2 = 50\n",
    "n_categories = 10\n",
    "\n",
    "eta_vals = np.logspace(-5, 0, 6)\n",
    "lmbd_vals = np.logspace(-5, 0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_data,\n",
    "        Y_data,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "        epochs=1,\n",
    "        batch_size=100,\n",
    "        eta=0.1,\n",
    "        lmbd=0.0,\n",
    "\n",
    "    ):\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "\n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.weights_layer1 = np.random.randn(self.n_features, self.n_neurons_layer1)\n",
    "        self.bias_layer1 = np.zeros(self.n_neurons_layer1)\n",
    "\n",
    "        self.weights_layer2 = np.random.randn(self.n_neurons_layer1, self.n_neurons_layer2)\n",
    "        self.bias_layer2 = np.zeros(self.n_neurons_layer2)\n",
    "\n",
    "        self.weights_output = np.random.randn(self.n_neurons_layer2, self.n_categories)\n",
    "        self.bias_output = np.zeros(self.n_categories)\n",
    "\n",
    "    def feed_forward(self):\n",
    "        self.z1 = np.dot(self.X_data, self.weights_layer1) + self.bias_layer1\n",
    "        self.a1 = expit(self.z1)\n",
    "\n",
    "        self.z2 = np.dot(self.a1, self.weights_layer2) + self.bias_layer2\n",
    "        self.a2 = expit(self.z2)\n",
    "\n",
    "        self.z3 = np.dot(self.a2, self.weights_output) + self.bias_output\n",
    "\n",
    "        exp_term = np.exp(self.z3)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "\n",
    "    def feed_forward_out(self, X):\n",
    "        z1 = np.dot(X, self.weights_layer1) + self.bias_layer1\n",
    "        a1 = expit(z1)\n",
    "\n",
    "        z2 = np.dot(a1, self.weights_layer2) + self.bias_layer2\n",
    "        a2 = expit(z2)\n",
    "\n",
    "        z3 = np.dot(a2, self.weights_output) + self.bias_output\n",
    "\n",
    "        exp_term = np.exp(z3)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "\n",
    "    def backpropagation(self):\n",
    "        error_output = self.probabilities\n",
    "        error_output[range(self.n_inputs), self.Y_data] -= 1\n",
    "        error_layer2 = np.dot(error_output, self.weights_output.T) * self.a2 * (1 - self.a2)\n",
    "        error_layer1 = np.dot(error_layer2, self.weights_layer2.T) * self.a1 * (1 - self.a1)\n",
    "\n",
    "        self.weights_output_gradient = np.dot(self.a2.T, error_output)\n",
    "        self.bias_output_gradient = np.sum(error_output)\n",
    "\n",
    "        self.weights_layer2_gradient = np.dot(self.a1.T, error_layer2)\n",
    "        self.bias_layer2_gradient = np.sum(error_layer2)\n",
    "\n",
    "        self.weights_layer1_gradient = np.dot(self.X_data.T, error_layer1)\n",
    "        self.bias_layer1_gradient = np.sum(error_layer1)\n",
    "\n",
    "        if self.lmbd > 0.0:\n",
    "            self.weights_output_gradient += self.lmbd * self.weights_output\n",
    "            self.weights_layer2_gradient += self.lmbd * self.weights_layer2\n",
    "            self.weights_layer1_gradient += self.lmbd * self.weights_layer1\n",
    "\n",
    "        self.weights_output -= self.eta * self.weights_output_gradient\n",
    "        self.bias_output -= self.eta * self.bias_output_gradient\n",
    "        self.weights_layer2 -= self.eta * self.weights_layer2_gradient\n",
    "        self.bias_layer2 -= self.eta * self.bias_layer2_gradient\n",
    "        self.weights_layer1 -= self.eta * self.weights_layer1_gradient\n",
    "        self.bias_layer1 -= self.eta * self.bias_layer1_gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.n_inputs = self.X_data.shape[0]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1e-05\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.1\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.10833333333333334\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.11666666666666667\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.08055555555555556\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.11944444444444445\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.10555555555555556\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.12777777777777777\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.11944444444444445\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.12222222222222222\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.1111111111111111\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.14166666666666666\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.225\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.25833333333333336\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.2111111111111111\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.225\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.325\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.7055555555555556\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.6861111111111111\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.6916666666666667\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.5194444444444445\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.725\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.6888888888888889\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.13333333333333333\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.08888888888888889\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.07222222222222222\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.08888888888888889\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.1\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.12222222222222222\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.13333333333333333\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DNN_numpy = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = NeuralNetwork(X_train, Y_train, eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n",
    "                                  n_neurons_layer1=n_neurons_layer1, n_neurons_layer2=n_neurons_layer2,\n",
    "                                  n_categories=n_categories)\n",
    "        dnn.train()\n",
    "        \n",
    "        DNN_numpy[i][j] = dnn\n",
    "        \n",
    "        test_predict = dnn.predict(X_test)\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", accuracy_score(Y_test, test_predict))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate  =  1e-05\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.2\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.11388888888888889\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.06944444444444445\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.42777777777777776\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.09722222222222222\n",
      "\n",
      "Learning rate  =  1e-05\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.1\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.8638888888888889\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.8722222222222222\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.8777777777777778\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.8444444444444444\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.8694444444444445\n",
      "\n",
      "Learning rate  =  0.0001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.8555555555555555\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9777777777777777\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9861111111111112\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9888888888888889\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9833333333333333\n",
      "\n",
      "Learning rate  =  0.001\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9777777777777777\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.975\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.9722222222222222\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.9777777777777777\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.9833333333333333\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.9777777777777777\n",
      "\n",
      "Learning rate  =  0.01\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.9722222222222222\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.3\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.18888888888888888\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.24722222222222223\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.7027777777777777\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.8305555555555556\n",
      "\n",
      "Learning rate  =  0.1\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.7361111111111112\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1e-05\n",
      "Accuracy score on test set:  0.07777777777777778\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.0001\n",
      "Accuracy score on test set:  0.11944444444444445\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.001\n",
      "Accuracy score on test set:  0.1\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.01\n",
      "Accuracy score on test set:  0.13333333333333333\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  0.1\n",
      "Accuracy score on test set:  0.13333333333333333\n",
      "\n",
      "Learning rate  =  1.0\n",
      "Lambda =  1.0\n",
      "Accuracy score on test set:  0.11944444444444445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "DNN_scikit = np.zeros((len(eta_vals), len(lmbd_vals)), dtype=object)\n",
    "\n",
    "for i, eta in enumerate(eta_vals):\n",
    "    for j, lmbd in enumerate(lmbd_vals):\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=(n_neurons_layer1, n_neurons_layer2), activation='logistic',\n",
    "                            alpha=lmbd, learning_rate_init=eta, max_iter=100)\n",
    "        dnn.fit(X_train, Y_train)\n",
    "        \n",
    "        DNN_scikit[i][j] = dnn\n",
    "        \n",
    "        print(\"Learning rate  = \", eta)\n",
    "        print(\"Lambda = \", lmbd)\n",
    "        print(\"Accuracy score on test set: \", dnn.score(X_test, Y_test))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_DNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.n_features), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_DNN(self):\n",
    "        with tf.name_scope('DNN'):\n",
    "            \n",
    "            # Fully connected layer 1\n",
    "            W_fc1 = self.weight_variable([self.n_features, self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            b_fc1 = self.bias_variable([self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            a_fc1 = tf.nn.sigmoid(tf.matmul(self.X, W_fc1) + b_fc1)\n",
    "            \n",
    "            # Fully connected layer 2\n",
    "            W_fc2 = self.weight_variable([self.n_neurons_layer1, self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            b_fc2 = self.bias_variable([self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            a_fc2 = tf.nn.sigmoid(tf.matmul(a_fc1, W_fc2) + b_fc2)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_layer2, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc2, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-04a3a7064218>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Train accuracy: 0.994\n",
      "Test accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "DNN = DNNModel(n_features, n_neurons_layer1, n_neurons_layer2, n_categories, eta=0.1)\n",
    "\n",
    "n_inputs = X_train.shape[0]\n",
    "iterations = n_inputs // batch_size\n",
    "data_indices = np.arange(n_inputs)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([DNN.loss, DNN.optimizer],\n",
    "                     feed_dict={DNN.X: batch_X,\n",
    "                                DNN.Y: batch_Y})\n",
    "            accuracy = sess.run(DNN.accuracy,\n",
    "                                feed_dict={DNN.X: batch_X,\n",
    "                                           DNN.Y: batch_Y})\n",
    "            step = sess.run(DNN.global_step)\n",
    "    \n",
    "    train_loss, train_accuracy = sess.run([DNN.loss, DNN.accuracy],\n",
    "                                          feed_dict={DNN.X: X_train,\n",
    "                                                     DNN.Y: Y_train})\n",
    "    \n",
    "    test_loss, test_accuracy = sess.run([DNN.loss, DNN.accuracy],\n",
    "                                        feed_dict={DNN.X: X_test,\n",
    "                                                   DNN.Y: Y_test})\n",
    "    \n",
    "    print(\"Train accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %.3f\" % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
