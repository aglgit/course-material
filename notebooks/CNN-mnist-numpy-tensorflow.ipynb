{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()\n",
    "    \n",
    "mnist_data = digits.images\n",
    "mnist_data = mnist_data[:,np.newaxis,:,:]\n",
    "labels = digits.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork:\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.n_inputs, self.depth, self.input_width, self.input_height = X_data.shape\n",
    "        \n",
    "        self.eta = 1E-6\n",
    "        self.lmbd = 0.1\n",
    "        \n",
    "        self.padding = 1\n",
    "        self.receptive_field = 3\n",
    "        self.stride = 1\n",
    "        self.downsampling = 2\n",
    "        \n",
    "        self.n_filters_conv = 3\n",
    "        self.n_neurons_flattened = int(self.n_filters_conv*self.input_width*self.input_height)\n",
    "        self.n_neurons_connected = 10\n",
    "        self.n_categories = 10\n",
    "        \n",
    "        self.output_width = int((self.input_width - self.receptive_field + 2 * self.padding) / self.stride + 1)\n",
    "        self.output_height = int((self.input_height - self.receptive_field + 2 * self.padding) / self.stride + 1)\n",
    "        self.downsampling_width = int(self.output_width / self.downsampling)\n",
    "        self.downsampling_height = int(self.output_height / self.downsampling)\n",
    "        \n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.weights_conv = np.random.randn(self.n_filters_conv, self.depth, self.receptive_field, self.receptive_field)\n",
    "        self.bias_conv = np.zeros(self.n_filters_conv)\n",
    "        \n",
    "        self.weights_connected = np.random.randn(self.n_neurons_flattened, self.n_neurons_connected)\n",
    "        self.bias_connected = np.zeros(self.n_neurons_connected)\n",
    "        \n",
    "        self.weights_output = np.random.randn(self.n_neurons_connected, self.n_categories)\n",
    "        self.bias_output = np.zeros(self.n_categories)\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        # Convolution layer\n",
    "        self.z1 = np.zeros((self.n_inputs, self.n_filters_conv, self.output_width, self.output_height))\n",
    "        for n in range(self.n_inputs):\n",
    "            X_padded = np.pad(self.X_data[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = h*self.stride\n",
    "                        h2 = h*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.z1[n, f, w, h] = np.sum(matrix_slice*self.weights_conv[f,:,:,:])\n",
    "        self.a1 = np.maximum(self.z1, 0)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        self.z2 = np.zeros_like(self.a1)\n",
    "        for n in range(self.n_inputs):\n",
    "            for w in range(self.downsampling_width):\n",
    "                for h in range(self.downsampling_height):\n",
    "                    w1 = w*self.downsampling\n",
    "                    w2 = w*self.downsampling + self.downsampling\n",
    "                    h1 = h*self.downsampling\n",
    "                    h2 = h*self.downsampling + self.downsampling\n",
    "                    matrix_slice = self.a1[n,:,w1:w2,h1:h2]\n",
    "                    matrix_slice[matrix_slice != matrix_slice.max()] = 0\n",
    "                    self.z2[n,:,w1:w2,h1:h2] = matrix_slice\n",
    "        self.a2 = np.maximum(self.z2, 0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.downsampled_array = self.a2.reshape(-1, self.n_neurons_flattened)\n",
    "        self.z3 = np.dot(self.downsampled_array, self.weights_connected) + self.bias_connected\n",
    "        self.a3 = np.maximum(self.z3, 0)\n",
    "        \n",
    "        # Output layer\n",
    "        exp_term = np.exp(self.a3)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        self.probabilities[np.abs(self.probabilities) < 1E-16] = 0\n",
    "        \n",
    "    def feed_forward_out(self, X):\n",
    "        # Convolution layer\n",
    "        n_inputs = X.shape[0]\n",
    "        z1 = np.zeros((n_inputs, self.n_filters_conv, self.output_width, self.output_height))\n",
    "        for n in range(n_inputs):\n",
    "            X_padded = np.pad(X[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = h*self.stride\n",
    "                        h2 = h*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        z1[n, f, w, h] = np.sum(matrix_slice*self.weights_conv[f,:,:,:])\n",
    "        a1 = np.maximum(z1, 0)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        z2 = np.zeros_like(a1)\n",
    "        for n in range(n_inputs):\n",
    "            for w in range(self.downsampling_width):\n",
    "                for h in range(self.downsampling_height):\n",
    "                    w1 = w*self.downsampling\n",
    "                    w2 = w*self.downsampling + self.downsampling\n",
    "                    h1 = h*self.downsampling\n",
    "                    h2 = h*self.downsampling + self.downsampling\n",
    "                    matrix_slice = a1[n,:,w1:w2,h1:h2]\n",
    "                    matrix_slice[matrix_slice != matrix_slice.max()] = 0\n",
    "                    z2[n,:,w1:w2,h1:h2] = matrix_slice\n",
    "        a2 = np.maximum(z2, 0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        downsampled_array = a2.reshape(-1, self.n_neurons_flattened)\n",
    "        z3 = np.dot(downsampled_array, self.weights_connected) + self.bias_connected\n",
    "        a3 = np.maximum(z3, 0)\n",
    "        \n",
    "        # Output layer\n",
    "        exp_term = np.exp(a3)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        probabilities[np.abs(probabilities) < 1E-16] = 0\n",
    "        return probabilities\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        # Output layer\n",
    "        error_output = self.probabilities\n",
    "        error_output[range(self.n_inputs), self.Y_data] -= 1\n",
    "        self.weights_output_gradient = np.dot(self.a3.T, error_output)\n",
    "        self.bias_output_gradient = np.sum(error_output)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        error_connected = np.dot(error_output, self.weights_output.T)*(self.a3 >=0)\n",
    "        self.weights_connected_gradient = np.dot(self.downsampled_array.T, error_connected)\n",
    "        self.bias_connected_gradient = np.sum(error_connected)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        error_downsampling = np.dot(error_connected, self.weights_connected.T)*(self.downsampled_array >=0)\n",
    "        error_downsampling = error_downsampling.reshape(-1, self.n_filters_conv, self.output_width, self.output_height)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        error_conv = error_downsampling\n",
    "        self.weights_conv_gradient = np.zeros_like(self.weights_conv)\n",
    "        for n in range(self.n_inputs):\n",
    "            X_padded = np.pad(self.X_data[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = h*self.stride\n",
    "                        h2 = h*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.weights_conv_gradient[f,:,:,:] += X_padded[:,w1:w2,h1:h2]*error_conv[n, f, w, h]\n",
    "        self.bias_conv_gradient = np.sum(error_conv)\n",
    "        \n",
    "        if self.lmbd > 0.0:\n",
    "            self.weights_output_gradient += self.lmbd * self.weights_output\n",
    "            self.weights_connected_gradient += self.lmbd * self.weights_connected\n",
    "            self.weights_conv_gradient += self.lmbd * self.weights_conv\n",
    "        \n",
    "        self.weights_output -= self.eta * self.weights_output_gradient\n",
    "        self.bias_output -= self.eta * self.bias_output_gradient\n",
    "        self.weights_connected -= self.eta * self.weights_connected_gradient\n",
    "        self.bias_connected -= self.eta * self.bias_connected_gradient\n",
    "        self.weights_conv -= self.eta * self.weights_conv_gradient\n",
    "        self.bias_conv -= self.eta * self.bias_conv_gradient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09722222222222222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7ca552b626455b9d2e126cc18f29a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.09722222222222222\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cnn = ConvolutionalNeuralNetwork(X_train, Y_train)\n",
    "pred = cnn.predict(X_test)\n",
    "print(accuracy_score(pred, Y_test))\n",
    "\n",
    "for i in tqdm_notebook(range(10)):\n",
    "    cnn.feed_forward()\n",
    "    cnn.backpropagation()\n",
    "\n",
    "pred = cnn.predict(X_test)\n",
    "print(accuracy_score(pred, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width,\n",
    "        input_height,\n",
    "        n_filters=10,\n",
    "        n_neurons_connected=50,\n",
    "        n_categories=10,\n",
    "        depth=1,\n",
    "        receptive_field=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.n_downsampled = int(input_width*input_height*n_filters / 4)\n",
    "        self.n_neurons_connected = n_neurons_connected\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.receptive_field = receptive_field\n",
    "        self.stride = stride\n",
    "        self.strides = [self.stride, self.stride, self.stride, self.stride]\n",
    "        self.padding = padding\n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_CNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.input_width, self.input_height, self.depth), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_CNN(self):\n",
    "        with tf.name_scope('CNN'):\n",
    "            \n",
    "            # Convolutional layer\n",
    "            W_conv = self.weight_variable([self.receptive_field, self.receptive_field, self.depth, self.n_filters], name='conv', dtype=tf.float32)\n",
    "            b_conv = self.weight_variable([self.n_filters], name='conv', dtype=tf.float32)\n",
    "            z_conv = tf.nn.conv2d(self.X, W_conv, self.strides, padding='SAME', name='conv') + b_conv\n",
    "            a_conv = tf.nn.relu(z_conv)\n",
    "            \n",
    "            # 2x2 max pooling\n",
    "            a_pool = tf.nn.max_pool(a_conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            \n",
    "            # Fully connected layer\n",
    "            a_pool_flat = tf.reshape(a_pool, [-1, self.n_downsampled])\n",
    "            W_fc = self.weight_variable([self.n_downsampled, self.n_neurons_connected], name='fc', dtype=tf.float32)\n",
    "            b_fc = self.bias_variable([self.n_neurons_connected], name='fc', dtype=tf.float32)\n",
    "            a_fc = tf.nn.relu(tf.matmul(a_pool_flat, W_fc) + b_fc)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_connected, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "mnist_data = mnist_data[:,0,:,:]\n",
    "mnist_data = mnist_data[:,:,:,np.newaxis]\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-726ef61e97ec>:71: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Train accuracy: 1.000\n",
      "Test accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "CNN = CNNModel(8, 8)\n",
    "\n",
    "n_inputs = X_train.shape[0]\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "iterations = n_inputs // batch_size\n",
    "data_indices = np.arange(n_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([CNN.loss, CNN.optimizer],\n",
    "                     feed_dict={CNN.X: batch_X,\n",
    "                                CNN.Y: batch_Y})\n",
    "            accuracy = sess.run(CNN.accuracy,\n",
    "                                feed_dict={CNN.X: batch_X,\n",
    "                                           CNN.Y: batch_Y})\n",
    "            step = sess.run(CNN.global_step)\n",
    "    \n",
    "    train_loss, train_accuracy = sess.run([CNN.loss, CNN.accuracy],\n",
    "                                          feed_dict={CNN.X: X_train,\n",
    "                                                     CNN.Y: Y_train})\n",
    "    \n",
    "    test_loss, test_accuracy = sess.run([CNN.loss, CNN.accuracy],\n",
    "                                        feed_dict={CNN.X: X_test,\n",
    "                                                   CNN.Y: Y_test})\n",
    "    \n",
    "    print(\"Train accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %.3f\" % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
