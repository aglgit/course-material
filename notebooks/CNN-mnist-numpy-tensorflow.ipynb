{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAABpCAYAAAAa0MmDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACR5JREFUeJzt3V2IXGcdx/HvPy9Squ0msSJa82ZLhXqxCSpVWs2iBasiSSgFodokUtG7TVD0chNJtXiViFpfLkysokWFLlpsL+xurFasbxvEC0VqSlMstphNmiLWlseLM0mnm8xzds/MzpzHfD8wdDfPnnOe89+Z3x7O/PtMpJSQJJVjxagnIElaGoNbkgpjcEtSYQxuSSqMwS1JhTG4JakwrQ/uiJiNiDuHvW3bWZcLWZMLWZOLK70uQwvuiDgRETcP63hLFRFfj4izXY//RMRzQzhu2+vykYj4S0Scjoh/RsTRiLhymY/Z9prsiojfR8SZiDgZEV+KiFXLfMy21yQi4mBEPNV5rsxGxFuHcNxW16VbRDwcEWkQz5XWX3EPS0rpUyml15x7AN8HfjjqebXAr4AbU0pjwJuBVcDB0U5p5C4H9gJXATcA7wM+M9IZjd5twMeBdwPrgF8D9450Ri0SEbdTvXYGYuTBHRFrI+KnEfFMRJzqfP2mBT92TUQ81vlLPh0R67q2f2dEPBoR8xFxPCImBjCnVwO3Akf73Vcfc2hFXVJKT6aUnu36p5eAa5vsq18tqsk9KaVHUkovpJSeAr4H3Nj8zJprS02AzcAvU0qPp5ReAr4LXN9wX31rUV2IiDFgCvhs030sNPLgpprDt4GNwAbg38BXFvzMHVR/zd8IvAh8GSAirgYeoLoCXEd11fPjiHjdwoNExIbOL2HDIuZ0K/AM8IsmJzQgralLRNwUEaeB56hqc6i/U2usNTVZ4D3An5d8NoPRlpr8ALg2Iq6LiNXALuDBPs+tH22pC8AXgHuAp/s5oVdIKQ3lAZwAbl7Ez20BTnV9Pwvc3fX99cALwErgc8C9C7Z/CNjVte2dDeb6c2C/dblgDlcD+4HrrMn5fewBTgJXXco1AV4FHAYSVQj+Hdi8nDUppC5vB+aobpNs6tRnVb/nPfIr7oi4PCK+ERFPRMQZqqvcNRGxsuvHnuz6+glgNdX9xY3AbZ2/ePMRMQ/cBLyhj/msB7YB32m6j0FoW10AUnVb4EGqq6uha1tNImIHcDfwgfTK20lD06KaTAHvANYDlwEHgIcj4vIG++pbG+oSESuArwGTKaUX+zmfhZb1nfBF+jTwFuCGlNLTEbEF+CMQXT+zvuvrDcB/gWepCn9vSukTA5zPHcCjKaXHB7jPJtpWl3NWAdcsw34XozU1iYhbgG8BH0op/WkQ+2yoLTUZB+5LKZ3sfH8kIg5RXcn+bgD7X6o21OVKqivu+yICqqt5gJMRcVtK6ZGmOx72FffqiLis67EKuILq/tN8582BqYts99GIuL7z1/vzwI/Sy2+AfDgi3h8RKzv7nLjImxBLcQdwpI/tm2htXSLi9s59vIiIjcBdVLeSlluba/Jeqjckb00pPdb4DJeutTUBfkt1lfr6iFgRER+juoL9W6MzXZq21uU01f3zLZ3HBzv//jbgN0s/zS7LfQ9qwb2otOBxsHNis8BZ4K/AJ+m6D9QZ+yLwGHAG+Ald9xOp2rGOAf+iekPxAWDDwntRVH9Rz54b6zHHdwHPA1dYl/P7uYvqHu7znf9+E3jtJV6TGar7uGe7Hj+7xGtyGfBV4B+d4/wBuOVSf/0smOsmBnSPOzo7lCQVYuRvTkqSlsbglqTCGNySVBiDW5IKY3BLUmGW63/AadSqcuTIkez4nj17muyWycnJnmOHDvW17EbU/8h5PWty4sSJnhvt3bs3u9Pp6eklTOFlU1MXa2td3DHXrFmTG172mmzZsiW709OnT/ccGxsb6zmWO2bNOddZ9prUPY8PHz7cc2x8fLx+Vhdx//33Z8c3bdqUG15KTaBhpszNzWXHJyYmGm1bc279WFRdvOKWpMIY3JJUGINbkgpjcEtSYQxuSSqMwS1JhWnDetzn7d+/Pzuea+vLte7k2qjabn5+Pjueq0nOgQMHeo7V1auubXMQcueda+GCfNvojh07eo7lnkN1xxyGujbNnJmZmZ5juda23Guy7nmyjC1zi1ZXs1xraa4FNNd+2c/vabG84pakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFaVU7YN1KXrn2nFwLTt1qcqOWa5uanZ1tvN/ctrnV4vpcCW8gcr+zulXpcnKrA+ZWFWyDXCtjXYtm7jmWa+vLjbWhRRLyczx27Fh221ybZK4VMvfash1QknQBg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVplV93HVyvaq5vuRdu3Y1Pubu3bsbbzsMuR7WXN9vTt0nhrdd0973bdu2DXYiA9bPczHXc53r2++nZ35Ycrmwffv27La5108uU06dOtVonzCY5W694pakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFaVU7YN1yiEePHu05tnHjxp5juU8MX7t2bf3EWix3brllSnP1aru6dqtc21zuOdSG5Wybqmv9zC1RmjvvNnxSe51c+2fd/HOthLnXSK7edc+jQbRYesUtSYUxuCWpMAa3JBXG4JakwhjcklQYg1uSCtOqdsCdO3dmx3NtNP20BJUs92no4+PjPceOHz/ec2xubq7xMYeh7vi5Vq261eJKVbeiY67Vtu4T4tsu136Xa/+s07QdcBgrinrFLUmFMbglqTAGtyQVxuCWpMIY3JJUGINbkgrTqnbAuhXrcm19/68tf3UffJtrA8u1/E1NTfUcG3W7H8D09HTPsdyqh5Bvfcu1OuZWWqxb8W3UNat7nuzbt6/nWMmrIkK+nbGu1TFXl7pVKEfJK25JKozBLUmFMbglqTAGtyQVxuCWpMIY3JJUGINbkgrTqj7uul7Uuk+yLlWut7junHP9wzMzMz3HJiYmauc1SmNjY4233bp16wBnUqlbDnYQn9zdj1wPOsDk5GTPsVHPvV+5PvTNmzc33m8/ff3LzStuSSqMwS1JhTG4JakwBrckFcbglqTCGNySVJhIKY16DpKkJfCKW5IKY3BLUmEMbkkqjMEtSYUxuCWpMAa3JBXG4JakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVxuCWpMIY3JJUGINbkgpjcEtSYQxuSSqMwS1JhTG4JakwBrckFcbglqTC/A/n8bHwwBaK2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "%matplotlib inline\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(len(digits.images))\n",
    "display = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[display]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[display[i]])\n",
    "plt.show()\n",
    "    \n",
    "mnist_data = digits.images\n",
    "mnist_data = mnist_data[:,np.newaxis,:,:]\n",
    "labels = digits.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andreas/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork:\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.n_inputs, self.depth, self.input_width, self.input_height = X_data.shape\n",
    "        \n",
    "        self.eta = 1E-6\n",
    "        self.lmbd = 0.1\n",
    "        \n",
    "        self.padding = 1\n",
    "        self.receptive_field = 3\n",
    "        self.stride = 1\n",
    "        self.downsampling = 2\n",
    "        \n",
    "        self.n_filters_conv = 3\n",
    "        self.n_neurons_flattened = int(self.n_filters_conv*self.input_width*self.input_height)\n",
    "        self.n_neurons_connected = 10\n",
    "        self.n_categories = 10\n",
    "        \n",
    "        self.output_width = int((self.input_width - self.receptive_field + 2 * self.padding) / self.stride + 1)\n",
    "        self.output_height = int((self.input_height - self.receptive_field + 2 * self.padding) / self.stride + 1)\n",
    "        self.downsampling_width = int(self.output_width / self.downsampling)\n",
    "        self.downsampling_height = int(self.output_height / self.downsampling)\n",
    "        \n",
    "        self.create_biases_and_weights()\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.weights_conv = np.random.randn(self.n_filters_conv, self.depth, self.receptive_field, self.receptive_field)\n",
    "        self.bias_conv = np.zeros(self.n_filters_conv)\n",
    "        \n",
    "        self.weights_connected = np.random.randn(self.n_neurons_flattened, self.n_neurons_connected)\n",
    "        self.bias_connected = np.zeros(self.n_neurons_connected)\n",
    "        \n",
    "        self.weights_output = np.random.randn(self.n_neurons_connected, self.n_categories)\n",
    "        self.bias_output = np.zeros(self.n_categories)\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        # Convolution layer\n",
    "        self.z1 = np.zeros((self.n_inputs, self.n_filters_conv, self.output_width, self.output_height))\n",
    "        for n in range(self.n_inputs):\n",
    "            X_padded = np.pad(self.X_data[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = h*self.stride\n",
    "                        h2 = h*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.z1[n, f, w, h] = np.sum(matrix_slice*self.weights_conv[f,:,:,:])\n",
    "        self.a1 = np.maximum(self.z1, 0)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        self.z2 = np.zeros_like(self.a1)\n",
    "        for n in range(self.n_inputs):\n",
    "            for w in range(self.downsampling_width):\n",
    "                for h in range(self.downsampling_height):\n",
    "                    w1 = w*self.downsampling\n",
    "                    w2 = w*self.downsampling + self.downsampling\n",
    "                    h1 = h*self.downsampling\n",
    "                    h2 = h*self.downsampling + self.downsampling\n",
    "                    matrix_slice = self.a1[n,:,w1:w2,h1:h2]\n",
    "                    matrix_slice[matrix_slice != matrix_slice.max()] = 0\n",
    "                    self.z2[n,:,w1:w2,h1:h2] = matrix_slice\n",
    "        self.a2 = np.maximum(self.z2, 0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.downsampled_array = self.a2.reshape(-1, self.n_neurons_flattened)\n",
    "        self.z3 = np.dot(self.downsampled_array, self.weights_connected) + self.bias_connected\n",
    "        self.a3 = np.maximum(self.z3, 0)\n",
    "        \n",
    "        # Output layer\n",
    "        exp_term = np.exp(self.a3)\n",
    "        self.probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        self.probabilities[np.abs(self.probabilities) < 1E-16] = 0\n",
    "        \n",
    "    def feed_forward_out(self, X):\n",
    "        # Convolution layer\n",
    "        n_inputs = X.shape[0]\n",
    "        z1 = np.zeros((n_inputs, self.n_filters_conv, self.output_width, self.output_height))\n",
    "        for n in range(n_inputs):\n",
    "            X_padded = np.pad(X[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = h*self.stride\n",
    "                        h2 = h*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        z1[n, f, w, h] = np.sum(matrix_slice*self.weights_conv[f,:,:,:])\n",
    "        a1 = np.maximum(z1, 0)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        z2 = np.zeros_like(a1)\n",
    "        for n in range(n_inputs):\n",
    "            for w in range(self.downsampling_width):\n",
    "                for h in range(self.downsampling_height):\n",
    "                    w1 = w*self.downsampling\n",
    "                    w2 = w*self.downsampling + self.downsampling\n",
    "                    h1 = h*self.downsampling\n",
    "                    h2 = h*self.downsampling + self.downsampling\n",
    "                    matrix_slice = a1[n,:,w1:w2,h1:h2]\n",
    "                    matrix_slice[matrix_slice != matrix_slice.max()] = 0\n",
    "                    z2[n,:,w1:w2,h1:h2] = matrix_slice\n",
    "        a2 = np.maximum(z2, 0)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        downsampled_array = a2.reshape(-1, self.n_neurons_flattened)\n",
    "        z3 = np.dot(downsampled_array, self.weights_connected) + self.bias_connected\n",
    "        a3 = np.maximum(z3, 0)\n",
    "        \n",
    "        # Output layer\n",
    "        exp_term = np.exp(a3)\n",
    "        probabilities = exp_term / np.sum(exp_term, axis=1, keepdims=True)\n",
    "        probabilities[np.abs(probabilities) < 1E-16] = 0\n",
    "        return probabilities\n",
    "    \n",
    "    def backpropagation(self):\n",
    "        # Output layer\n",
    "        error_output = self.probabilities\n",
    "        error_output[range(self.n_inputs), self.Y_data] -= 1\n",
    "        self.weights_output_gradient = np.dot(self.a3.T, error_output)\n",
    "        self.bias_output_gradient = np.sum(error_output)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        error_connected = np.dot(error_output, self.weights_output.T)*(self.a3 >=0)\n",
    "        self.weights_connected_gradient = np.dot(self.downsampled_array.T, error_connected)\n",
    "        self.bias_connected_gradient = np.sum(error_connected)\n",
    "        \n",
    "        # 2x2 downsampling layer\n",
    "        error_downsampling = np.dot(error_connected, self.weights_connected.T)*(self.downsampled_array >=0)\n",
    "        error_downsampling = error_downsampling.reshape(-1, self.n_filters_conv, self.output_width, self.output_height)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        error_conv = error_downsampling\n",
    "        self.weights_conv_gradient = np.zeros_like(self.weights_conv)\n",
    "        for n in range(self.n_inputs):\n",
    "            X_padded = np.pad(self.X_data[n,:,:,:], ((0,0),(self.padding,self.padding),(self.padding,self.padding)), \n",
    "                              'constant')\n",
    "            for f in range(self.n_filters_conv):\n",
    "                for w in range(self.output_width):\n",
    "                    for h in range(self.output_height):\n",
    "                        w1 = w*self.stride\n",
    "                        w2 = w*self.stride + self.receptive_field\n",
    "                        h1 = h*self.stride\n",
    "                        h2 = h*self.stride + self.receptive_field\n",
    "                        matrix_slice = X_padded[:,w1:w2,h1:h2]\n",
    "                        self.weights_conv_gradient[f,:,:,:] += X_padded[:,w1:w2,h1:h2]*error_conv[n, f, w, h]\n",
    "        self.bias_conv_gradient = np.sum(error_conv)\n",
    "        \n",
    "        if self.lmbd > 0.0:\n",
    "            self.weights_output_gradient += self.lmbd * self.weights_output\n",
    "            self.weights_connected_gradient += self.lmbd * self.weights_connected\n",
    "            self.weights_conv_gradient += self.lmbd * self.weights_conv\n",
    "        \n",
    "        self.weights_output -= self.eta * self.weights_output_gradient\n",
    "        self.bias_output -= self.eta * self.bias_output_gradient\n",
    "        self.weights_connected -= self.eta * self.weights_connected_gradient\n",
    "        self.bias_connected -= self.eta * self.bias_connected_gradient\n",
    "        self.weights_conv -= self.eta * self.weights_conv_gradient\n",
    "        self.bias_conv -= self.eta * self.bias_conv_gradient\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_out(X)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08333333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124aad10a9924d129aa55ad2af75e05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.09722222222222222\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cnn = ConvolutionalNeuralNetwork(X_train, Y_train)\n",
    "pred = cnn.predict(X_test)\n",
    "print(accuracy_score(pred, Y_test))\n",
    "\n",
    "for i in tqdm_notebook(range(10)):\n",
    "    cnn.feed_forward()\n",
    "    cnn.backpropagation()\n",
    "\n",
    "pred = cnn.predict(X_test)\n",
    "print(accuracy_score(pred, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width,\n",
    "        input_height,\n",
    "        n_filters=10,\n",
    "        n_neurons_connected=50,\n",
    "        n_categories=10,\n",
    "        depth=1,\n",
    "        receptive_field=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.input_width = input_width\n",
    "        self.input_height = input_height\n",
    "        \n",
    "        self.n_filters = n_filters\n",
    "        self.n_downsampled = int(input_width*input_height*n_filters / 4)\n",
    "        self.n_neurons_connected = n_neurons_connected\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.receptive_field = receptive_field\n",
    "        self.stride = stride\n",
    "        self.strides = [self.stride, self.stride, self.stride, self.stride]\n",
    "        self.padding = padding\n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_CNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.input_width, self.input_height, self.depth), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_CNN(self):\n",
    "        with tf.name_scope('CNN'):\n",
    "            \n",
    "            # Convolutional layer\n",
    "            W_conv = self.weight_variable([self.receptive_field, self.receptive_field, self.depth, self.n_filters], name='conv', dtype=tf.float32)\n",
    "            b_conv = self.weight_variable([self.n_filters], name='conv', dtype=tf.float32)\n",
    "            z_conv = tf.nn.conv2d(self.X, W_conv, self.strides, padding='SAME', name='conv') + b_conv\n",
    "            a_conv = tf.nn.relu(z_conv)\n",
    "            \n",
    "            # 2x2 max pooling\n",
    "            a_pool = tf.nn.max_pool(a_conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME', name='pool')\n",
    "            \n",
    "            # Fully connected layer\n",
    "            a_pool_flat = tf.reshape(a_pool, [-1, self.n_downsampled])\n",
    "            W_fc = self.weight_variable([self.n_downsampled, self.n_neurons_connected], name='fc', dtype=tf.float32)\n",
    "            b_fc = self.bias_variable([self.n_neurons_connected], name='fc', dtype=tf.float32)\n",
    "            a_fc = tf.nn.relu(tf.matmul(a_pool_flat, W_fc) + b_fc)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_connected, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "mnist_data = mnist_data[:,0,:,:]\n",
    "mnist_data = mnist_data[:,:,:,np.newaxis]\n",
    "\n",
    "train_to_test_ratio = 0.8\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mnist_data, labels, train_size=train_to_test_ratio)\n",
    "\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-726ef61e97ec>:71: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Train accuracy: 1.000\n",
      "Test accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "CNN = CNNModel(8, 8)\n",
    "\n",
    "n_inputs = X_train.shape[0]\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "iterations = n_inputs // batch_size\n",
    "data_indices = np.arange(n_inputs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([CNN.loss, CNN.optimizer],\n",
    "                     feed_dict={CNN.X: batch_X,\n",
    "                                CNN.Y: batch_Y})\n",
    "            accuracy = sess.run(CNN.accuracy,\n",
    "                                feed_dict={CNN.X: batch_X,\n",
    "                                           CNN.Y: batch_Y})\n",
    "            step = sess.run(CNN.global_step)\n",
    "    \n",
    "    train_loss, train_accuracy = sess.run([CNN.loss, CNN.accuracy],\n",
    "                                          feed_dict={CNN.X: X_train,\n",
    "                                                     CNN.Y: Y_train})\n",
    "    \n",
    "    test_loss, test_accuracy = sess.run([CNN.loss, CNN.accuracy],\n",
    "                                        feed_dict={CNN.X: X_test,\n",
    "                                                   CNN.Y: Y_test})\n",
    "    \n",
    "    print(\"Train accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %.3f\" % test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
