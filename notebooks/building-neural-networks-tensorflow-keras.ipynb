{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building neural networks in Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "  \n",
    "Tensorflow is an open source library machine learning library developed by the Google Brain team for internal use. It was released under the Apache 2.0 open source license in November 9, 2015.  \n",
    "  \n",
    "Tensorflow is a computational framework that allows you to construct machine learning models at different levels of abstraction, from high-level, object-oriented APIs like Keras, down to the C++ kernels that Tensorflow is built upon. The higher levels of abstraction are simpler to use, but less flexible, and your choice of implementation should reflect the problem you are trying to solve.  \n",
    "  \n",
    "Tensorflow uses a [dataflow graph](https://www.tensorflow.org/guide/graphs) to represent your computation\n",
    "in terms of the dependencies between individual operations, such that you first build a Tensorflow **graph** to represent your model, and then create a Tensorflow **session** to run the graph.\n",
    "  \n",
    "In this guide we will analyze the same data as we did in our NumPy and scikit-learn tutorial, gathered\n",
    "from the MNIST database of images. We will give an introduction to the lower level Python APIs, and see how we use them to build our graph. Then we will build (effectively) the same graph in Keras, to see just how simple solving a machine learning problem can be.  \n",
    "  \n",
    "If you prefer you can also take a look at the official guides on [low level APIs](https://www.tensorflow.org/guide/low_level_intro) and [Keras](https://www.tensorflow.org/guide/keras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs = (n_inputs, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "labels = (n_inputs) = (1797,)\n",
      "X = (n_inputs, n_features) = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# display images in notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(\"inputs = (n_inputs, pixel_width, pixel_height) = \" + str(inputs.shape))\n",
    "print(\"labels = (n_inputs) = \" + str(labels.shape))\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(\"X = (n_inputs, n_features) = \" + str(inputs.shape))\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(\"Label: %d\" % digits.target[random_indices[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "# one-liner from scikit-learn library\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DNNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features,\n",
    "        n_neurons_layer1=100,\n",
    "        n_neurons_layer2=50,\n",
    "        n_categories=2,\n",
    "        eta=0.1,\n",
    "    ):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.n_neurons_layer1 = n_neurons_layer1\n",
    "        self.n_neurons_layer2 = n_neurons_layer2\n",
    "        self.n_categories = n_categories\n",
    "        \n",
    "        self.eta = eta\n",
    "        \n",
    "        self.create_placeholders()\n",
    "        self.create_DNN()\n",
    "        self.create_loss()\n",
    "        self.create_accuracy()\n",
    "        self.create_optimiser()\n",
    "    \n",
    "    def create_placeholders(self):\n",
    "        # placeholders are fine here, but Datasets are the preferred method\n",
    "        # of streaming data into a  model\n",
    "        with tf.name_scope('data'):\n",
    "            self.X = tf.placeholder(tf.float32, shape=(None, self.n_features), name='X_data')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=(None, self.n_categories), name='Y_data')\n",
    "    \n",
    "    def create_DNN(self):\n",
    "        with tf.name_scope('DNN'):\n",
    "            \n",
    "            # Fully connected layer 1\n",
    "            W_fc1 = self.weight_variable([self.n_features, self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            b_fc1 = self.bias_variable([self.n_neurons_layer1], name='fc1', dtype=tf.float32)\n",
    "            a_fc1 = tf.nn.sigmoid(tf.matmul(self.X, W_fc1) + b_fc1)\n",
    "            \n",
    "            # Fully connected layer 2\n",
    "            W_fc2 = self.weight_variable([self.n_neurons_layer1, self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            b_fc2 = self.bias_variable([self.n_neurons_layer2], name='fc2', dtype=tf.float32)\n",
    "            a_fc2 = tf.nn.sigmoid(tf.matmul(a_fc1, W_fc2) + b_fc2)\n",
    "            \n",
    "            # Output layer\n",
    "            W_out = self.weight_variable([self.n_neurons_layer2, self.n_categories], name='out', dtype=tf.float32)\n",
    "            b_out = self.bias_variable([self.n_categories], name='out', dtype=tf.float32)\n",
    "            self.z_out = tf.matmul(a_fc2, W_out) + b_out\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.name_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.z_out))\n",
    "\n",
    "    def create_accuracy(self):\n",
    "        with tf.name_scope('accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.Y,1), tf.argmax(self.z_out, 1))\n",
    "            correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "            self.accuracy = tf.reduce_mean(correct_prediction)\n",
    "    \n",
    "    def create_optimiser(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.eta).minimize(self.loss, global_step=self.global_step)\n",
    "            \n",
    "    def weight_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)\n",
    "    \n",
    "    def bias_variable(self, shape, name='', dtype=tf.float32):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial, name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 2\n",
    "batch_size = 100\n",
    "n_features = X_train.shape[1]\n",
    "n_neurons_layer1 = 100\n",
    "n_neurons_layer2 = 50\n",
    "n_categories = 10\n",
    "\n",
    "eta_vals = np.logspace(-5, 0, 6)\n",
    "lmbd_vals = np.logspace(-5, 0, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-04a3a7064218>:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Train accuracy: 0.998\n",
      "Test accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "DNN = DNNModel(n_features, n_neurons_layer1, n_neurons_layer2, n_categories, eta=0.1)\n",
    "\n",
    "n_inputs = X_train.shape[0]\n",
    "iterations = n_inputs // batch_size\n",
    "data_indices = np.arange(n_inputs)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        for j in range(iterations):\n",
    "            chosen_datapoints = np.random.choice(data_indices, size=batch_size, replace=False)\n",
    "            batch_X, batch_Y = X_train[chosen_datapoints], Y_train[chosen_datapoints]\n",
    "            \n",
    "            sess.run([DNN.loss, DNN.optimizer],\n",
    "                     feed_dict={DNN.X: batch_X,\n",
    "                                DNN.Y: batch_Y})\n",
    "            accuracy = sess.run(DNN.accuracy,\n",
    "                                feed_dict={DNN.X: batch_X,\n",
    "                                           DNN.Y: batch_Y})\n",
    "            step = sess.run(DNN.global_step)\n",
    "    \n",
    "    train_loss, train_accuracy = sess.run([DNN.loss, DNN.accuracy],\n",
    "                                          feed_dict={DNN.X: X_train,\n",
    "                                                     DNN.Y: Y_train})\n",
    "    \n",
    "    test_loss, test_accuracy = sess.run([DNN.loss, DNN.accuracy],\n",
    "                                        feed_dict={DNN.X: X_test,\n",
    "                                                   DNN.Y: Y_test})\n",
    "    \n",
    "    print(\"Train accuracy: %.3f\" % train_accuracy)\n",
    "    print(\"Test accuracy: %.3f\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('logs/')\n",
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 61us/step\n",
      "Accuracy: 0.897\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "def create_DNN(n_features, n_neurons_layer1, n_neurons_layer2, n_categories):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_neurons_layer1, input_dim=n_features, activation='sigmoid'))\n",
    "    model.add(Dense(n_neurons_layer1, activation='sigmoid'))\n",
    "    model.add(Dense(n_categories, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_DNN(n_features, n_neurons_layer1, n_neurons_layer2, n_categories)\n",
    "model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "scores = model.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f\" % scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
